{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c187a4f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-10T16:38:23.803180Z",
     "iopub.status.busy": "2024-09-10T16:38:23.802536Z",
     "iopub.status.idle": "2024-09-10T16:38:24.381923Z",
     "shell.execute_reply": "2024-09-10T16:38:24.379595Z"
    },
    "papermill": {
     "duration": 0.603392,
     "end_time": "2024-09-10T16:38:24.384940",
     "exception": false,
     "start_time": "2024-09-10T16:38:23.781548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/gensim-doc2vec-model/__results__.html\n",
      "/kaggle/input/gensim-doc2vec-model/__notebook__.ipynb\n",
      "/kaggle/input/gensim-doc2vec-model/__output__.json\n",
      "/kaggle/input/gensim-doc2vec-model/model_dim_128_epoch_40.bin\n",
      "/kaggle/input/gensim-doc2vec-model/custom.css\n",
      "/kaggle/input/geotext/user_info.dev.txt\n",
      "/kaggle/input/geotext/user_info.test.txt\n",
      "/kaggle/input/geotext/full_text.txt\n",
      "/kaggle/input/geotext/model_dim_128_epoch_40.bin\n",
      "/kaggle/input/geotext/user_info.train.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0482d1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:38:24.420620Z",
     "iopub.status.busy": "2024-09-10T16:38:24.420012Z",
     "iopub.status.idle": "2024-09-10T16:38:24.427800Z",
     "shell.execute_reply": "2024-09-10T16:38:24.425456Z"
    },
    "papermill": {
     "duration": 0.031188,
     "end_time": "2024-09-10T16:38:24.432037",
     "exception": false,
     "start_time": "2024-09-10T16:38:24.400849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pdb\n",
    "#from scipy._lib.six import xrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e7298",
   "metadata": {
    "papermill": {
     "duration": 0.020528,
     "end_time": "2024-09-10T16:38:24.470343",
     "exception": false,
     "start_time": "2024-09-10T16:38:24.449815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b22806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:38:24.506721Z",
     "iopub.status.busy": "2024-09-10T16:38:24.506102Z",
     "iopub.status.idle": "2024-09-10T16:38:24.540035Z",
     "shell.execute_reply": "2024-09-10T16:38:24.538454Z"
    },
    "papermill": {
     "duration": 0.055896,
     "end_time": "2024-09-10T16:38:24.544345",
     "exception": false,
     "start_time": "2024-09-10T16:38:24.488449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KDTree:\n",
    "    def __init__(self, bucket_size, dimensions, parent=None):\n",
    "        self.bucket_size = bucket_size\n",
    "        self.parent = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.split_dimension = None\n",
    "        self.split_value = None\n",
    "        self.index_locations = []\n",
    "        self.location_count = 0\n",
    "        self.min_limit = [np.Inf] * dimensions \n",
    "        self.max_limit = [-np.Inf] * dimensions\n",
    "        self.dimensions = dimensions\n",
    "    \n",
    "    def get_leaf(self, location):\n",
    "        if not self.left and not self.right:\n",
    "            return self\n",
    "        elif location[self.split_dimension] <= self.split_value:\n",
    "            return self.left.get_leaf(location)\n",
    "        else:\n",
    "            return self.right.get_leaf(location) \n",
    "    \n",
    "    def add_point(self, index_location_tuple):\n",
    "        self.index_locations.append(index_location_tuple)\n",
    "        self.location_count += 1\n",
    "        self.extendBounds(index_location_tuple[1])\n",
    "        self.min_boundary = copy.deepcopy(self.min_limit)\n",
    "        self.max_boundary = copy.deepcopy(self.max_limit)\n",
    "        \n",
    "    def extendBounds(self, location):\n",
    "        # empty\n",
    "        if self.min_limit == None:\n",
    "            self.min_limit = copy.deepcopy(location)\n",
    "            self.max_limit = copy.deepcopy(location)\n",
    "            return\n",
    "        for i in range(self.dimensions):\n",
    "            self.min_limit[i] = min(self.min_limit[i], location[i])\n",
    "            self.max_limit[i] = max(self.max_limit[i], location[i])\n",
    "    \n",
    "    def findWidestAxis(self):\n",
    "        widths = [self.max_limit[i] - self.min_limit[i] for i in range(self.dimensions)]\n",
    "        widest_axis = np.argmax(widths)\n",
    "        return widest_axis\n",
    "\n",
    "    def getNodes(self):\n",
    "        nodes = []\n",
    "        self.getNodesHelper(nodes)\n",
    "        return nodes\n",
    "    \n",
    "    def getNodesHelper(self, nodes):\n",
    "        nodes.append(self)\n",
    "        if self.left:\n",
    "            self.left.getNodesHelper(nodes)\n",
    "        if self.right:\n",
    "            self.right.getNodesHelper(nodes)\n",
    "    \n",
    "    def getLeaves(self):\n",
    "        leaves = []\n",
    "        self.getLeavesHelper(leaves)\n",
    "        return leaves\n",
    "    \n",
    "    def getLeavesHelper(self, leaves):\n",
    "        if not self.right and not self.left:\n",
    "            leaves.append(self)\n",
    "        else:\n",
    "            if self.left:\n",
    "                self.left.getLeavesHelper(leaves)\n",
    "            if self.right:\n",
    "                self.right.getLeavesHelper(leaves)\n",
    "                \n",
    "    def balance(self):\n",
    "        self.nodeSplit(self)\n",
    "    \n",
    "    def nodeSplit(self, cursor, empty_non_leaf=True):\n",
    "        if cursor.location_count > cursor.bucket_size:\n",
    "            cursor.split_dimension = cursor.findWidestAxis()\n",
    "            # the partition method is the median of all values in the widest dimension\n",
    "            cursor.split_value = np.median([cursor.index_locations[i][1][cursor.split_dimension] for i in range(cursor.location_count)])\n",
    "            # if width is 0 (all the values are the same) don't partition\n",
    "            if cursor.min_limit[cursor.split_dimension] == cursor.max_limit[cursor.split_dimension]:\n",
    "                return\n",
    "            # Don't let the split value be the same as the upper value as\n",
    "            # can happen due to rounding errors!\n",
    "            if cursor.split_value == cursor.max_limit[cursor.split_dimension]:\n",
    "                cursor.split_value = cursor.min_limit[cursor.split_dimension]\n",
    "            cursor.left = KDTree(bucket_size=cursor.bucket_size, dimensions=cursor.dimensions, parent=cursor)\n",
    "            cursor.right = KDTree(bucket_size=cursor.bucket_size, dimensions=cursor.dimensions, parent=cursor)\n",
    "            \n",
    "            cursor.left.min_boundary = copy.deepcopy(cursor.min_boundary)\n",
    "            cursor.left.max_boundary = copy.deepcopy(cursor.max_boundary)\n",
    "            cursor.right.min_boundary = copy.deepcopy(cursor.min_boundary)\n",
    "            cursor.right.max_boundary = copy.deepcopy(cursor.max_boundary)\n",
    "            cursor.left.max_boundary[cursor.split_dimension] = cursor.split_value\n",
    "            cursor.right.min_boundary[cursor.split_dimension] = cursor.split_value\n",
    "            \n",
    "            for index_loc in cursor.index_locations:\n",
    "                if index_loc[1][cursor.split_dimension] > cursor.split_value:\n",
    "                    cursor.right.index_locations.append(index_loc)\n",
    "                    cursor.right.location_count += 1\n",
    "                    cursor.right.extendBounds(index_loc[1])\n",
    "                else:\n",
    "                    cursor.left.index_locations.append(index_loc)\n",
    "                    cursor.left.location_count += 1\n",
    "                    cursor.left.extendBounds(index_loc[1])\n",
    "            if empty_non_leaf:\n",
    "                cursor.index_locations = []\n",
    "            cursor.nodeSplit(cursor.left)\n",
    "            cursor.nodeSplit(cursor.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "667eec19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:38:24.580364Z",
     "iopub.status.busy": "2024-09-10T16:38:24.579679Z",
     "iopub.status.idle": "2024-09-10T16:38:24.596980Z",
     "shell.execute_reply": "2024-09-10T16:38:24.594036Z"
    },
    "papermill": {
     "duration": 0.039623,
     "end_time": "2024-09-10T16:38:24.600940",
     "exception": false,
     "start_time": "2024-09-10T16:38:24.561317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class KDTreeClustering:\n",
    "    def __init__(self, bucket_size=10):\n",
    "        self.bucket_size = bucket_size\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X):\n",
    "        # X is an array\n",
    "        if hasattr(X, 'shape'):\n",
    "            n_samples = X.shape[0]\n",
    "            dimensions = X.shape[1]\n",
    "        else:\n",
    "            n_samples = len(X)\n",
    "            dimensions = len(X[0])\n",
    "        \n",
    "        self.kdtree = KDTree(bucket_size=self.bucket_size, dimensions=dimensions, parent=None)\n",
    "        for i in range(n_samples):\n",
    "            self.kdtree.add_point((i, X[i]))\n",
    "        self.kdtree.nodeSplit(cursor=self.kdtree, empty_non_leaf=True)\n",
    "        self.clusters = [leave.index_locations for leave in self.kdtree.getLeaves()]\n",
    "        clusters = [cluster.index_locations for cluster in self.kdtree.getLeaves()]\n",
    "        results = np.zeros((n_samples,), dtype=int)\n",
    "        for i, id_locs in enumerate(clusters):\n",
    "            for id, l in id_locs:\n",
    "                results[id] = i\n",
    "        self.clusters = results\n",
    "        self.num_clusters = len(clusters)\n",
    "        self.is_fitted = True\n",
    "           \n",
    "    def get_clusters(self):\n",
    "        if self.is_fitted:\n",
    "            return self.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be52d5f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:38:24.634192Z",
     "iopub.status.busy": "2024-09-10T16:38:24.633715Z",
     "iopub.status.idle": "2024-09-10T16:38:24.646753Z",
     "shell.execute_reply": "2024-09-10T16:38:24.644265Z"
    },
    "papermill": {
     "duration": 0.034025,
     "end_time": "2024-09-10T16:38:24.650833",
     "exception": false,
     "start_time": "2024-09-10T16:38:24.616808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"if __name__ == '__main__':\\n    # tree = KDTree(300, 2)\\n    import params\\n    import geolocate\\n    \\n    geolocate.initialize(granularity=params.BUCKET_SIZE, write=False, readText=True, reload_init=False, regression=False)\\n    locations = [geolocate.locationStr2Float(loc) for loc in params.trainUsers.values()]\\n    clusterer = KDTreeClustering(bucket_size=params.BUCKET_SIZE)\\n    clusterer.fit(locations)\\n    clusters = clusterer.get_clusters()\\n    \\n    pdb.set_trace()\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"if __name__ == '__main__':\n",
    "    # tree = KDTree(300, 2)\n",
    "    import params\n",
    "    import geolocate\n",
    "    \n",
    "    geolocate.initialize(granularity=params.BUCKET_SIZE, write=False, readText=True, reload_init=False, regression=False)\n",
    "    locations = [geolocate.locationStr2Float(loc) for loc in params.trainUsers.values()]\n",
    "    clusterer = KDTreeClustering(bucket_size=params.BUCKET_SIZE)\n",
    "    clusterer.fit(locations)\n",
    "    clusters = clusterer.get_clusters()\n",
    "    \n",
    "    pdb.set_trace()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7cd5b",
   "metadata": {
    "papermill": {
     "duration": 0.015054,
     "end_time": "2024-09-10T16:38:24.682144",
     "exception": false,
     "start_time": "2024-09-10T16:38:24.667090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a84b01bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:38:24.718069Z",
     "iopub.status.busy": "2024-09-10T16:38:24.717368Z",
     "iopub.status.idle": "2024-09-10T16:38:25.285508Z",
     "shell.execute_reply": "2024-09-10T16:38:25.283536Z"
    },
    "papermill": {
     "duration": 0.591503,
     "end_time": "2024-09-10T16:38:25.288774",
     "exception": false,
     "start_time": "2024-09-10T16:38:24.697271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def normalized_laplacian(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return (sp.eye(adj.shape[0]) - d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)).tocoo()\n",
    "\n",
    "\n",
    "def laplacian(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    row_sum = np.array(adj.sum(1)).flatten()\n",
    "    d_mat = sp.diags(row_sum)\n",
    "    return (d_mat - adj).tocoo()\n",
    "\n",
    "\n",
    "def gcn(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return (sp.eye(adj.shape[0]) + d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)).tocoo()\n",
    "\n",
    "\n",
    "def aug_normalized_adjacency(adj):\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def normalized_adjacency(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return (d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)).tocoo()\n",
    "\n",
    "\n",
    "def random_walk_laplacian(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    d_inv = np.power(row_sum, -1.0).flatten()\n",
    "    d_mat = sp.diags(d_inv)\n",
    "    return (sp.eye(adj.shape[0]) - d_mat.dot(adj)).tocoo()\n",
    "\n",
    "\n",
    "def aug_random_walk(adj):\n",
    "    adj = adj + sp.eye(adj.shape[0])\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    d_inv = np.power(row_sum, -1.0).flatten()\n",
    "    d_mat = sp.diags(d_inv)\n",
    "    return d_mat.dot(adj).tocoo()\n",
    "\n",
    "\n",
    "def random_walk(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    row_sum = np.array(adj.sum(1))\n",
    "    d_inv = np.power(row_sum, -1.0).flatten()\n",
    "    d_mat = sp.diags(d_inv)\n",
    "    return d_mat.dot(adj).tocoo()\n",
    "\n",
    "\n",
    "def no_norm(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def fetch_normalization(type):\n",
    "    switcher = {\n",
    "       'NormLap': normalized_laplacian,  # A' = I - D^-1/2 * A * D^-1/2\n",
    "       'Lap': laplacian,  # A' = D - A\n",
    "       'RWalkLap': random_walk_laplacian,  # A' = I - D^-1 * A\n",
    "       'FirstOrderGCN': gcn,   # A' = I + D^-1/2 * A * D^-1/2\n",
    "       'AugNormAdj': aug_normalized_adjacency,  # A' = (D + I)^-1/2 * ( A + I ) * (D + I)^-1/2\n",
    "       'NormAdj': normalized_adjacency,  # D^-1/2 * A * D^-1/2\n",
    "       'RWalk': random_walk,  # A' = D^-1*A\n",
    "       'AugRWalk': aug_random_walk,  # A' = (D + I)^-1*(A + I)\n",
    "       'NoNorm': no_norm,  # A' = A\n",
    "    }\n",
    "    func = switcher.get(type, lambda: \"Invalid normalization technique.\")\n",
    "    return func\n",
    "\n",
    "\n",
    "def row_normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6347e52",
   "metadata": {
    "papermill": {
     "duration": 0.016904,
     "end_time": "2024-09-10T16:38:25.323323",
     "exception": false,
     "start_time": "2024-09-10T16:38:25.306419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30579dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:38:25.364496Z",
     "iopub.status.busy": "2024-09-10T16:38:25.363786Z",
     "iopub.status.idle": "2024-09-10T16:38:29.280720Z",
     "shell.execute_reply": "2024-09-10T16:38:29.279479Z"
    },
    "papermill": {
     "duration": 3.943232,
     "end_time": "2024-09-10T16:38:29.283568",
     "exception": false,
     "start_time": "2024-09-10T16:38:25.340336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae516e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:38:29.316581Z",
     "iopub.status.busy": "2024-09-10T16:38:29.315809Z",
     "iopub.status.idle": "2024-09-10T16:38:29.351487Z",
     "shell.execute_reply": "2024-09-10T16:38:29.350236Z"
    },
    "papermill": {
     "duration": 0.055521,
     "end_time": "2024-09-10T16:38:29.354426",
     "exception": false,
     "start_time": "2024-09-10T16:38:29.298905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SGC(nn.Module):\n",
    "    \"\"\"\n",
    "    A Simple PyTorch Implementation of Logistic Regression.\n",
    "    Assuming the features have been preprocessed with k-step graph propagation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nclass):\n",
    "        super(SGC, self).__init__()\n",
    "\n",
    "        self.W = nn.Linear(nfeat, nclass)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.W(x)\n",
    "        return h1\n",
    "\n",
    "\n",
    "class SGC_multi_hid(nn.Module):\n",
    "    \"\"\"\n",
    "    Morton added.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nclass, dropout):\n",
    "        super(SGC_multi_hid, self).__init__()\n",
    "\n",
    "        self.W1 = nn.Linear(nfeat, 256, bias=True)\n",
    "        self.W2 = nn.Linear(256, nclass, bias=True)\n",
    "        # self.W3 = nn.Linear(600, nclass, bias=True)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.W1(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.W2(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # x = self.W3(x)\n",
    "        # x = F.dropout(x, self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    A Graph Convolution Layer (GCN)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.W = nn.Linear(in_features, out_features, bias=bias)\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "        stdv = 1. / math.sqrt(self.W.weight.size(1))\n",
    "        self.W.weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = self.W(input)\n",
    "        output = torch.spmm(adj, support)\n",
    "        return output\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Two-layer GCN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, 1024)\n",
    "        self.gc3 = GraphConvolution(1024, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj, use_relu=True):\n",
    "        x = self.gc1(x, adj)\n",
    "        if use_relu:\n",
    "            x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        if use_relu:\n",
    "            x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc3(x, adj)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Two-layer HGNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(HGNN, self).__init__()\n",
    "        print(\"HGNN model starting...\")\n",
    "\n",
    "        self.cluster_W_1 = nn.Linear(nfeat, nfeat, bias=True)\n",
    "        self.user_W_1 = nn.Linear(2 * nfeat, 2 * nfeat, bias=True)\n",
    "        self.cluster_W_2 = nn.Linear(2 * nfeat, 2 * nfeat, bias=True)\n",
    "        self.user_W_2 = nn.Linear(4 * nfeat, 4 * nfeat, bias=True)\n",
    "        self.output_W = nn.Linear(4 * nfeat, nclass, bias=True)\n",
    "        self.dropout = dropout\n",
    "        # self.cluster_feat_new = Variable(torch.zeros(nclass, nfeat))\n",
    "\n",
    "    def forward(self, features, node2cluster_arr, cluster_nodes=None, cluster_adj=None):\n",
    "        if cluster_nodes is None and cluster_adj is None:\n",
    "            feat_with_hops_1 = torch.mm(node2cluster_arr, self.cluster_feat_new_1)\n",
    "            user_features_1 = self.user_W_1(torch.cat([features, feat_with_hops_1], dim=1))\n",
    "            feat_with_hops_2 = torch.mm(node2cluster_arr, self.cluster_feat_new_2)\n",
    "            user_features_2 = self.user_W_2(torch.cat([user_features_1, feat_with_hops_2], dim=1))\n",
    "\n",
    "            out = self.output_W(user_features_2)\n",
    "            out = F.dropout(out, self.dropout, training=self.training)\n",
    "            return out\n",
    "        else:\n",
    "            cluster_feat_1 = torch.mm(cluster_nodes, features)\n",
    "            self.cluster_feat_new_1 = self.cluster_W_1(torch.mm(cluster_adj, cluster_feat_1))\n",
    "            feat_with_hops_1 = torch.mm(node2cluster_arr, self.cluster_feat_new_1)\n",
    "            user_features_1 = self.user_W_1(torch.cat([features, feat_with_hops_1], dim=1))\n",
    "            # user_features = F.dropout(user_features, self.dropout, training=self.training)\n",
    "\n",
    "            cluster_feat_2 = torch.mm(cluster_nodes, user_features_1)\n",
    "            self.cluster_feat_new_2 = self.cluster_W_2(torch.mm(cluster_adj, cluster_feat_2))\n",
    "            feat_with_hops_2 = torch.mm(node2cluster_arr, self.cluster_feat_new_2)\n",
    "            user_features_2 = self.user_W_2(torch.cat([user_features_1, feat_with_hops_2], dim=1))\n",
    "            # user_features = F.dropout(user_features, self.dropout, training=self.training)\n",
    "\n",
    "            out = self.output_W(user_features_2)\n",
    "            # out = F.dropout(out, self.dropout, training=self.training)\n",
    "            return out\n",
    "\n",
    "\n",
    "def get_model(model_opt, nfeat, nclass, nhid=0, dropout=0.1, usecuda=False):\n",
    "    if model_opt == \"GCN\":\n",
    "        model = GCN(nfeat=nfeat,\n",
    "                    nhid=nhid,\n",
    "                    nclass=nclass,\n",
    "                    dropout=dropout)\n",
    "\n",
    "    elif model_opt == \"SGC\":\n",
    "        model = SGC(nfeat=nfeat,\n",
    "                    nclass=nclass)\n",
    "\n",
    "    elif model_opt == \"SGC_multi_hid\":\n",
    "        model = SGC_multi_hid(nfeat=nfeat,\n",
    "                              nclass=nclass,\n",
    "                              dropout=dropout)\n",
    "    elif model_opt == \"HGNN\":\n",
    "        model = HGNN(nfeat=nfeat,\n",
    "                     nhid=nhid,\n",
    "                     nclass=nclass,\n",
    "                     dropout=dropout)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError('model:{} is not implemented!'.format(model_opt))\n",
    "\n",
    "    if usecuda:\n",
    "        model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acba0a",
   "metadata": {
    "papermill": {
     "duration": 0.014629,
     "end_time": "2024-09-10T16:38:29.383392",
     "exception": false,
     "start_time": "2024-09-10T16:38:29.368763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DataProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7478e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:38:29.413758Z",
     "iopub.status.busy": "2024-09-10T16:38:29.413299Z",
     "iopub.status.idle": "2024-09-10T16:38:47.911600Z",
     "shell.execute_reply": "2024-09-10T16:38:47.910099Z"
    },
    "papermill": {
     "duration": 18.517012,
     "end_time": "2024-09-10T16:38:47.914493",
     "exception": false,
     "start_time": "2024-09-10T16:38:29.397481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hickle\r\n",
      "  Downloading hickle-5.0.3-py3-none-any.whl.metadata (22 kB)\r\n",
      "Requirement already satisfied: h5py>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from hickle) (3.11.0)\r\n",
      "Requirement already satisfied: numpy!=1.20,>=1.8 in /opt/conda/lib/python3.10/site-packages (from hickle) (1.26.4)\r\n",
      "Downloading hickle-5.0.3-py3-none-any.whl (107 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: hickle\r\n",
      "Successfully installed hickle-5.0.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install hickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "249d12b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:38:47.947591Z",
     "iopub.status.busy": "2024-09-10T16:38:47.947081Z",
     "iopub.status.idle": "2024-09-10T16:39:07.208348Z",
     "shell.execute_reply": "2024-09-10T16:39:07.207115Z"
    },
    "papermill": {
     "duration": 19.281547,
     "end_time": "2024-09-10T16:39:07.211438",
     "exception": false,
     "start_time": "2024-09-10T16:38:47.929891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import gzip\n",
    "import pickle\n",
    "import hickle\n",
    "#import kdtree\n",
    "import torch\n",
    "import scipy.sparse as spsp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from haversine import haversine\n",
    "#from scipy._lib.six import xrange\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from utils import sparse_mx_to_torch_sparse_tensor\n",
    "\n",
    "import smart_open\n",
    "import gensim\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "466168c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:07.246357Z",
     "iopub.status.busy": "2024-09-10T16:39:07.245490Z",
     "iopub.status.idle": "2024-09-10T16:39:07.387255Z",
     "shell.execute_reply": "2024-09-10T16:39:07.386062Z"
    },
    "papermill": {
     "duration": 0.163726,
     "end_time": "2024-09-10T16:39:07.390272",
     "exception": false,
     "start_time": "2024-09-10T16:39:07.226546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dump_obj(obj, filename, protocol=-1, serializer=pickle):\n",
    "    if serializer == hickle:\n",
    "        serializer.dump(obj, filename, mode='w', compression='gzip')\n",
    "    else:\n",
    "        with gzip.open(filename, 'wb') as fout:\n",
    "            serializer.dump(obj, fout, protocol)\n",
    "\n",
    "\n",
    "def load_obj(filename, serializer=pickle):\n",
    "    if serializer == hickle:\n",
    "        obj = serializer.load(filename)\n",
    "    else:\n",
    "        with gzip.open(filename, 'rb') as fin:\n",
    "            obj = serializer.load(fin)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def efficient_collaboration_weighted_projected_graph2(B, nodes):\n",
    "    # B:        the whole graph including known nodes and mentioned nodes   --large graph\n",
    "    # nodes:    the node_id of known nodes                                  --small graph node\n",
    "    nodes = set(nodes)\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    all_nodes = set(B.nodes())\n",
    "    for m in all_nodes:\n",
    "        nbrs = B[m]\n",
    "        target_nbrs = [t for t in nbrs if t in nodes]\n",
    "        # add edge between known nodesA(m) and known nodesB(n)\n",
    "        if m in nodes:\n",
    "            for n in target_nbrs:\n",
    "                if m < n:\n",
    "                    if not G.has_edge(m, n):\n",
    "                        # Morton added for exclude the long edges\n",
    "\n",
    "                        G.add_edge(m, n)\n",
    "        # add edge between known n1 and known n2,\n",
    "        # just because n1 and n2 have relation to m\n",
    "        for n1 in target_nbrs:\n",
    "            for n2 in target_nbrs:\n",
    "                if n1 < n2:\n",
    "                    if not G.has_edge(n1, n2):\n",
    "                        G.add_edge(n1, n2)\n",
    "    return G\n",
    "\n",
    "\n",
    "# normalization according to row, each row represent a feature\n",
    "def feature_normalization1(dt):\n",
    "    mean_num = np.mean(dt, axis=0)\n",
    "    sigma = np.std(dt, axis=0)\n",
    "    return (dt - mean_num) / sigma\n",
    "\n",
    "\n",
    "# normalization according to row, each row represent a feature\n",
    "def feature_normalization2(dt):\n",
    "    mean_num = np.mean(dt, axis=0)\n",
    "    max_num = np.max(dt, axis=0)\n",
    "    min_num = np.min(dt, axis=0)\n",
    "    return (dt - mean_num) / (max_num - min_num)\n",
    "\n",
    "\n",
    "def preprocess_data(data_args):\n",
    "    \"\"\" obtain the parameters \"\"\"\n",
    "    data_dir = data_args.dir\n",
    "    dump_file = data_args.dump_file\n",
    "    bucket_size = data_args.bucket\n",
    "    encoding = data_args.encoding\n",
    "    celebrity_threshold = data_args.celebrity\n",
    "    mindf = data_args.mindf\n",
    "    builddata = data_args.builddata\n",
    "    doc2vec_model_file = data_args.doc2vec_model_file\n",
    "    # vocab_file = os.path.join(data_dir, 'vocab.pkl')\n",
    "    if os.path.exists(dump_file):\n",
    "        if not builddata:\n",
    "            print('loading data from file : {}'.format(dump_file))\n",
    "            data = load_obj(dump_file)\n",
    "            return data\n",
    "    dl = DataLoader(data_home=data_dir, bucket_size=bucket_size, encoding=encoding,\n",
    "                    celebrity_threshold=celebrity_threshold, mindf=mindf, token_pattern=r'(?u)(?<![@])#?\\b\\w\\w+\\b')\n",
    "    dl.load_data()  # 'user'        df_train          df_dev          df_test\n",
    "    dl.assignClasses()  # 'lat', 'lon'  train_classes     dev_classes     test_class\n",
    "    # dl.tfidf()                # 'text'        X_train           X_dev           X_test        self.tf_idf_sum\n",
    "    dl.doc2vec_feature(doc2vec_model_file)  # 'text'        X_train           X_dev           X_test\n",
    "\n",
    "    # dl.encodingContent(vacab_size=80000, encod_size=512, padding=0)     # 'text'   X_train   X_dev      X_test\n",
    "    # vocab = dl.vectorizer.vocabulary_\n",
    "    # dump_obj(vocab, vocab_file)\n",
    "    # print('successfully dump vocab in {}'.format(vocab_file))\n",
    "    U_test = dl.df_test.index.tolist()\n",
    "    U_dev = dl.df_dev.index.tolist()\n",
    "    U_train = dl.df_train.index.tolist()\n",
    "\n",
    "    dl.get_graph()\n",
    "\n",
    "    X_train = dl.X_train\n",
    "    X_dev = dl.X_dev\n",
    "    X_test = dl.X_test\n",
    "    Y_train = dl.train_classes\n",
    "    Y_dev = dl.dev_classes\n",
    "    Y_test = dl.test_classes\n",
    "\n",
    "    P_test = [str(a[0]) + ',' + str(a[1]) for a in dl.df_test[['lat', 'lon']].values.tolist()]\n",
    "    P_train = [str(a[0]) + ',' + str(a[1]) for a in dl.df_train[['lat', 'lon']].values.tolist()]\n",
    "    P_dev = [str(a[0]) + ',' + str(a[1]) for a in dl.df_dev[['lat', 'lon']].values.tolist()]\n",
    "\n",
    "    classLatMedian = {str(c): dl.cluster_median[c][0] for c in dl.cluster_median}\n",
    "    classLonMedian = {str(c): dl.cluster_median[c][1] for c in dl.cluster_median}\n",
    "\n",
    "    userLocation = {}\n",
    "    for i, u in enumerate(U_train):\n",
    "        userLocation[u] = P_train[i]\n",
    "    for i, u in enumerate(U_test):\n",
    "        userLocation[u] = P_test[i]\n",
    "    for i, u in enumerate(U_dev):\n",
    "        userLocation[u] = P_dev[i]\n",
    "\n",
    "    adj = nx.adjacency_matrix(dl.graph)\n",
    "    print('adjacency matrix created.')\n",
    "\n",
    "    '''get training node index of each set.'''\n",
    "    print(\"---- get training node index of each set.\")\n",
    "    cluster_number = len(set(Y_train))\n",
    "    cluster_nodes = []\n",
    "    for set_idx in range(0, cluster_number):\n",
    "        #print(\"---- append cluster number:{}\".format(set_idx))\n",
    "        cluster_nodes.append(np.where(Y_train == set_idx)[0])\n",
    "    cluster_arr = np.zeros(shape=(cluster_number, Y_train.shape[0]))\n",
    "    for c_i, nodes in enumerate(cluster_nodes):\n",
    "        for j in nodes:\n",
    "            cluster_arr[c_i][j] = 1\n",
    "\n",
    "    '''build the cluster graph Adjacency Matrix.'''\n",
    "    print(\"---- build the cluster graph Adjacency Matrix.\")\n",
    "    cluster_adj = np.zeros((cluster_number, cluster_number))\n",
    "    for i in range(0, cluster_number):\n",
    "        for j in range(i + 1, cluster_number):\n",
    "            clui_loc = (classLatMedian[str(i)], classLonMedian[str(i)])\n",
    "            cluj_loc = (classLatMedian[str(j)], classLonMedian[str(j)])\n",
    "            dis = haversine(clui_loc, cluj_loc)\n",
    "            cluster_adj[i][j] = dis\n",
    "            cluster_adj[j][i] = dis\n",
    "    cluster_adj = (cluster_adj.max(axis=0) - cluster_adj) / cluster_adj.max(axis=0)\n",
    "\n",
    "    '''store the shortest path matrix of each pair of nodes.'''\n",
    "\n",
    "    # def Floyd(d):\n",
    "    #     n = d.shape[0]\n",
    "    #     for k in range(n):\n",
    "    #         for i in range(n):\n",
    "    #             for j in range(n):\n",
    "    #                 d[i][j] = min(d[i][j], d[i][k] + d[k][j])\n",
    "    #         if (k + 1) % 100 == 0:\n",
    "    #             print(k + 1, \"done.\")\n",
    "    #\n",
    "    #     return d\n",
    "    # shorted_matrix = Floyd(adj.A)\n",
    "\n",
    "    '''calculate the shortest hop path of every pair of nodes on the mention graph.'''\n",
    "    print(\"---- calculate the shortest hop path of every pair of nodes on the mention graph.\")\n",
    "    num_of_nodes = dl.graph.number_of_nodes()\n",
    "    print(\"---- num_of_nodes {}\".format(num_of_nodes))\n",
    "    shorted_matrix = np.zeros((num_of_nodes, num_of_nodes))\n",
    "    \n",
    "    from networkx import dijkstra_path_length\n",
    "    for node_i in range(0, num_of_nodes):\n",
    "        for node_j in range(node_i + 1, num_of_nodes):\n",
    "            try:\n",
    "                path_len = 1#dijkstra_path_length(dl.graph, source=node_i, target=node_j)\n",
    "                #print(path_len)\n",
    "            except Exception as e:\n",
    "                shorted_matrix[node_i][node_j] = 100\n",
    "                #print(\"---- shorted_matrix. node_i{}, node_j{}\".format(i, j))\n",
    "                continue\n",
    "            shorted_matrix[node_i][node_j] = path_len\n",
    "        if (node_i + 1) % 1000 == 0:\n",
    "            print(node_i + 1, \"shortest path done.\")\n",
    "        #print(\"----Node {} done\".format(node_i))\n",
    "\n",
    "    '''calculate the shortest hop path on the mention graph.'''\n",
    "    print(\"calculate the shortest hop path on the mention graph.\")\n",
    "    num_of_nodes = dl.graph.number_of_nodes()\n",
    "    node2cluster_arr = np.zeros((num_of_nodes, cluster_number))  # size=(num_of_nodes, cluster_number)\n",
    "    for node_i in range(0, num_of_nodes):\n",
    "        for cluster_j in range(0, cluster_number):\n",
    "            if node_i in cluster_nodes[cluster_j]:\n",
    "                '''node_i belong the cluster_j (only exist for training nodes)'''\n",
    "                shortest_path = 0\n",
    "            else:\n",
    "                nodes = cluster_nodes[cluster_j]\n",
    "                path_list = []\n",
    "                for node_j in nodes:\n",
    "                    path_len = shorted_matrix[min(node_i, node_j)][max(node_i, node_j)]\n",
    "                    if path_len == 100:\n",
    "                        continue\n",
    "                    path_list.append(path_len)\n",
    "                if len(path_list) == 0:\n",
    "                    '''for the isolate nodes.'''\n",
    "                    shortest_path = 10\n",
    "                else:\n",
    "                    shortest_path = np.mean(path_list)\n",
    "\n",
    "            node2cluster_arr[node_i][cluster_j] = shortest_path + 1\n",
    "\n",
    "        if (node_i + 1) % 1000 == 0:\n",
    "            print(node_i + 1, \"have gotten the shortest path.\")\n",
    "\n",
    "    '''save into files in order to repeat calculate.'''\n",
    "    print(\"save into files in order to repeat calculate.\")\n",
    "    data = (adj, X_train, Y_train, X_dev, Y_dev, X_test, Y_test, U_train, U_dev, U_test,\n",
    "            classLatMedian, classLonMedian, userLocation, cluster_arr, cluster_adj, node2cluster_arr)\n",
    "    dump_obj(data, dump_file)\n",
    "    print('successfully dump data in {}'.format(str(dump_file)))\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_data(data, args):\n",
    "    adj, X_train, Y_train, X_dev, Y_dev, X_test, Y_test, U_train, U_dev, U_test, \\\n",
    "    classLatMedian, classLonMedian, userLocation, cluster_nodes, cluster_adj, node2cluster_arr = data\n",
    "\n",
    "    '''porting to pyTorch and concat the matrix'''\n",
    "    normalization = args.normalization\n",
    "    adj_normalizer = fetch_normalization(normalization)\n",
    "    adj = adj_normalizer(adj)\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj).float()\n",
    "\n",
    "    # '''word embedding for features'''\n",
    "    # X_train = sparse_mx_to_torch_sparse_tensor(X_train)  # torch.FloatTensor(X_train.data)\n",
    "    # X_dev = sparse_mx_to_torch_sparse_tensor(X_dev)\n",
    "    # X_test = sparse_mx_to_torch_sparse_tensor(X_test)\n",
    "    # features = torch.cat((X_train, X_dev, X_test), 0)\n",
    "    # features = torch.FloatTensor(features.to_dense())\n",
    "\n",
    "    # features = np.vstack((X_train.toarray(), X_dev.toarray(), X_test.toarray()))\n",
    "    features = np.vstack((X_train, X_dev, X_test))  # only for dec2vec_feature\n",
    "\n",
    "    '''feature normalization'''\n",
    "    if args.feature_norm == 'Standard':\n",
    "        features = feature_normalization1(features)\n",
    "        print(\"Standard: using feature_normalization1 ..\")\n",
    "    elif args.feature_norm == 'Mean':\n",
    "        features = feature_normalization2(features)\n",
    "        print(\"Mean: using feature_normalization2 ..\")\n",
    "    else:\n",
    "        print(\"none feature normalization.\")\n",
    "\n",
    "    features = torch.FloatTensor(features)\n",
    "    cluster_nodes = torch.FloatTensor(cluster_nodes)\n",
    "    cluster_adj = torch.FloatTensor(cluster_adj)\n",
    "    node2cluster_arr = torch.FloatTensor(node2cluster_arr)\n",
    "    print(\"feature shape:{}\".format(features.shape))\n",
    "\n",
    "    '''get labels'''\n",
    "    labels = torch.LongTensor(np.hstack((Y_train, Y_dev, Y_test)))\n",
    "\n",
    "    '''get index of train val and test'''\n",
    "    len_train = int(X_train.shape[0])\n",
    "    len_val = int(X_dev.shape[0])\n",
    "    len_test = int(X_test.shape[0])\n",
    "    idx_train = torch.LongTensor(range(len_train))\n",
    "    idx_val = torch.LongTensor(range(len_train, len_train + len_val))\n",
    "    idx_test = torch.LongTensor(range(len_train + len_val, len_train + len_val + len_test))\n",
    "\n",
    "    '''convert to cuda'''\n",
    "    if args.usecuda:\n",
    "        print(\"converting data to CUDA format...\")\n",
    "        adj = adj.cuda()\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        idx_train = idx_train.cuda()\n",
    "        idx_val = idx_val.cuda()\n",
    "        idx_test = idx_test.cuda()\n",
    "\n",
    "    data = (adj, features, labels, idx_train, idx_val, idx_test, U_train, U_dev, U_test,\n",
    "            classLatMedian, classLonMedian, userLocation, cluster_nodes, cluster_adj, node2cluster_arr)\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if tokens_only:\n",
    "                yield gensim.utils.simple_preprocess(line)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])\n",
    "\n",
    "\n",
    "def similarity_weight_of_graph(whole_graph, doc2vec_model_file):\n",
    "    # load model\n",
    "    model = gensim.models.doc2vec.Doc2Vec.load(doc2vec_model_file)\n",
    "    # load feature process file.\n",
    "    # all_corpus_file = \"./data/cmu/train_corpus/cmu_all_process.txt\"\n",
    "    # all_corpus = list(read_corpus(all_corpus_file))\n",
    "\n",
    "    # start to modify the weight of edges.\n",
    "    sim_list = list()\n",
    "    edge_list = whole_graph.edges\n",
    "    for i, item in enumerate(edge_list):\n",
    "        # wm_dis = model.wv.wmdistance(all_corpus[item[0]].words, all_corpus[item[1]].words)\n",
    "        sim = model.docvecs.similarity(item[0], item[1])\n",
    "        if sim <= 0:\n",
    "            whole_graph.add_edge(item[0], item[1], weight=abs(sim) + 1)\n",
    "        else:\n",
    "            whole_graph.add_edge(item[0], item[1], weight=sim * 10)\n",
    "        sim_list.append(sim)\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(\"{} is finished!\".format(i + 1))\n",
    "\n",
    "    print(\"min:{} \\t max:{} \\t mean:{}\".format(np.min(sim_list), np.max(sim_list), np.mean(sim_list)))\n",
    "\n",
    "    return whole_graph\n",
    "\n",
    "\n",
    "def route_weight_of_graph(whole_graph):\n",
    "    ret_graph = whole_graph\n",
    "    edge_list = whole_graph.edges\n",
    "    count = 0\n",
    "    for item in edge_list:\n",
    "        whole_graph.remove_edge(item[0], item[1])\n",
    "        try:\n",
    "            route_path = nx.shortest_path_length(G=whole_graph, source=item[0], target=item[1])\n",
    "        except nx.NetworkXNoPath:\n",
    "            route_path = 0\n",
    "            count += 1\n",
    "            print(\"No path.{}\".format(count))\n",
    "        whole_graph.add_edge(item[0], item[1])\n",
    "        if route_path == 0:\n",
    "            ret_graph.add_edge(item[0], item[1], weight=1)\n",
    "        else:\n",
    "            ret_graph.add_edge(item[0], item[1], weight=(1 / route_path) + 1)\n",
    "\n",
    "    return ret_graph\n",
    "\n",
    "\n",
    "def guess_virtual_nodes(whole_graph, userLocation, U_train, U_dev, U_test):\n",
    "    index_to_userId = list(U_train + U_dev + U_test)\n",
    "    edge_list_all = list(whole_graph.edges)\n",
    "    edge_list = list()\n",
    "    guess_nodes = list()  # {<(id_1,id1),(id_1,id2)>,<(id_2,id1),(id_2,id2),...>,...}\n",
    "    node_temp = list()\n",
    "\n",
    "    # remain the edges between train and val/test data.\n",
    "    index_threshold = len(U_train)\n",
    "    for item in edge_list_all:\n",
    "        if (item[0] < index_threshold) and (item[1] >= index_threshold):\n",
    "            edge_list.append(item[::-1])\n",
    "\n",
    "    # sort by item[0]\n",
    "    edge_list.sort(key=lambda x: x[0])\n",
    "\n",
    "    A_index = edge_list[0][0]\n",
    "    for item in edge_list:\n",
    "        if item[0] == A_index:\n",
    "            node_temp.append(item)\n",
    "        else:\n",
    "            guess_nodes.append(node_temp)\n",
    "            node_temp = []\n",
    "            node_temp.append(item)\n",
    "            A_index = item[0]\n",
    "    guess_nodes.append(node_temp)  # add the last node.\n",
    "    print(\"guess_nodes data format:{<(id_1,id1),(id_1,id2)>,<(id_2,id1),(id_2,id2),...>,...}\")\n",
    "\n",
    "    # guess a virtual position of nodes.\n",
    "    neighbour_lati = list()\n",
    "    neighbour_long = list()\n",
    "    for node in guess_nodes:\n",
    "        for item in node:\n",
    "            user_ID = index_to_userId[item[1]]  # get user_ID\n",
    "            user_loca = userLocation[user_ID].split(',')  # get user_location\n",
    "            neighbour_lati.append(float(user_loca[0]))\n",
    "            neighbour_long.append(float(user_loca[1]))\n",
    "        # updata the target node location information\n",
    "        user_ID = index_to_userId[item[0]]\n",
    "        userLocation[user_ID] = str(np.mean(neighbour_lati)) + ',' + str(np.mean(neighbour_long))\n",
    "        neighbour_lati.clear()\n",
    "        neighbour_long.clear()\n",
    "    print(\"len(guess_nodes) is:{}\".format(len(guess_nodes)))\n",
    "    return userLocation\n",
    "\n",
    "\n",
    "def add_lineEXP_only_for_train_data(whole_graph, userLocation, U_train, dis_mean):\n",
    "    index_to_userId = list(U_train)\n",
    "    edge_list_old = list(whole_graph.edges)\n",
    "\n",
    "    '''only select train nodes and reproduce new edge_list'''\n",
    "    edge_list = list()\n",
    "    train_len = len(index_to_userId)\n",
    "    for item in edge_list_old:\n",
    "        if (item[0] < train_len) and (item[1] < train_len):\n",
    "            edge_list.append(item)\n",
    "        else:\n",
    "            continue\n",
    "    print(\"len of edge_list is:{}\".format(len(edge_list)))\n",
    "\n",
    "    for item in edge_list:\n",
    "        # get the index of nodes in the whole graph\n",
    "        A_index = item[0]\n",
    "        B_index = item[1]\n",
    "        # get user_id\n",
    "        use_id_A = index_to_userId[A_index]\n",
    "        use_id_B = index_to_userId[B_index]\n",
    "        # get the location of user using user_id\n",
    "        locationA = userLocation[use_id_A].split(',')\n",
    "        locationB = userLocation[use_id_B].split(',')\n",
    "        # calculate the distance\n",
    "        distance = haversine((float(locationA[0]), float(locationA[1])),\n",
    "                             (float(locationB[0]), float(locationB[1])))\n",
    "        if distance <= dis_mean:\n",
    "            weight = (dis_mean + 1 - distance) / 10\n",
    "        else:\n",
    "            weight = math.exp(dis_mean - distance)\n",
    "        whole_graph.add_edge(A_index, B_index, weight=weight)\n",
    "    return whole_graph\n",
    "\n",
    "\n",
    "def add_lineEXP_value_of_edges(whole_graph, userLocation, U_train, U_dev, U_test, dis_mean):\n",
    "    index_to_userId = list(U_train + U_dev + U_test)\n",
    "    edge_list = list(whole_graph.edges)\n",
    "    for item in edge_list:\n",
    "        # get the index of nodes in the whole graph\n",
    "        A_index = item[0]\n",
    "        B_index = item[1]\n",
    "        # get user_id\n",
    "        use_id_A = index_to_userId[A_index]\n",
    "        use_id_B = index_to_userId[B_index]\n",
    "        # get the location of user using user_id\n",
    "        locationA = userLocation[use_id_A].split(',')\n",
    "        locationB = userLocation[use_id_B].split(',')\n",
    "        # calculate the distance\n",
    "        distance = haversine((float(locationA[0]), float(locationA[1])),\n",
    "                             (float(locationB[0]), float(locationB[1])))\n",
    "        if distance <= dis_mean:\n",
    "            weight = dis_mean + 1 - distance\n",
    "        else:\n",
    "            weight = math.exp(dis_mean - distance)\n",
    "        whole_graph.add_edge(A_index, B_index, weight=weight)\n",
    "    return whole_graph\n",
    "\n",
    "\n",
    "def add_RBF_value_of_edges(whole_graph, userLocation, U_train, U_dev, U_test, dis_mean, dis_var):\n",
    "    index_to_userId = list(U_train + U_dev + U_test)\n",
    "    edge_list = list(whole_graph.edges)\n",
    "    for item in edge_list:\n",
    "        # get the index of nodes in the whole graph\n",
    "        A_index = item[0]\n",
    "        B_index = item[1]\n",
    "        # get user_id\n",
    "        use_id_A = index_to_userId[A_index]\n",
    "        use_id_B = index_to_userId[B_index]\n",
    "        # get the location of user using user_id\n",
    "        locationA = userLocation[use_id_A].split(',')\n",
    "        locationB = userLocation[use_id_B].split(',')\n",
    "        # calculate the distance\n",
    "        distance = haversine((float(locationA[0]), float(locationA[1])),\n",
    "                             (float(locationB[0]), float(locationB[1])))\n",
    "        weight = (distance - dis_mean) / dis_var\n",
    "        whole_graph.add_edge(A_index, B_index, weight=weight)\n",
    "    return whole_graph\n",
    "\n",
    "\n",
    "def del_long_edge_from_graph(whole_graph, userLocation, U_train, U_dev, U_test, threshold=2800):\n",
    "    index_to_userId = list(U_train + U_dev + U_test)\n",
    "    edge_list = list(whole_graph.edges)\n",
    "    remove_count = 0\n",
    "    for item in edge_list:\n",
    "        # get the index of nodes in the whole graph\n",
    "        A_index = item[0]\n",
    "        B_index = item[1]\n",
    "        # get user_id\n",
    "        use_id_A = index_to_userId[A_index]\n",
    "        use_id_B = index_to_userId[B_index]\n",
    "        # get the location of user using user_id\n",
    "        locationA = userLocation[use_id_A].split(',')\n",
    "        locationB = userLocation[use_id_B].split(',')\n",
    "        # calculate the distance\n",
    "        distance = haversine((float(locationA[0]), float(locationA[1])),\n",
    "                             (float(locationB[0]), float(locationB[1])))\n",
    "        if distance >= threshold:\n",
    "            whole_graph.remove_edge(A_index, B_index)\n",
    "            remove_count += 1\n",
    "    print(\"remove {} edges with threshold {}\".format(remove_count, threshold))\n",
    "\n",
    "    return whole_graph\n",
    "\n",
    "\n",
    "def del_nodes_from_dataset(data_input, del_index_list):\n",
    "    if type(data_input).__name__ == 'csr_matrix':\n",
    "        # print(\"origin shape of csr_matrix:{}\".format(data_input.shape))\n",
    "        data_input = data_input.toarray()\n",
    "        for i in range(len(del_index_list) - 1, -1, -1):\n",
    "            data_input = np.delete(data_input, del_index_list[i], 0)\n",
    "        data_input = spsp.csr_matrix(data_input)\n",
    "        # print(\"now shape:{}\\tdel_list len:{}\".format((data_input.shape), len(del_index_list)))\n",
    "\n",
    "    elif type(data_input).__name__ == 'ndarray':\n",
    "        # print(\"origin shape of ndarray:{}\".format(data_input.shape))\n",
    "        data_input = data_input.tolist()\n",
    "        for i in range(len(del_index_list) - 1, -1, -1):\n",
    "            del data_input[del_index_list[i]]\n",
    "        data_input = np.array(data_input)\n",
    "        # print(\"now shape:{}\\tdel_list len:{}\".format((data_input.shape), len(del_index_list)))\n",
    "\n",
    "    else:\n",
    "        # print(\"origin len:{}\".format(len(data_input)))\n",
    "        for i in range(len(del_index_list) - 1, -1, -1):\n",
    "            del data_input[del_index_list[i]]\n",
    "        # print(\"now len:{}\\tdel_list len:{}\".format(len(data_input), len(del_index_list)))\n",
    "\n",
    "    return data_input\n",
    "\n",
    "\n",
    "def del_isolated_nodes_from_graph(whole_graph):\n",
    "    isolated_nodes = list()\n",
    "    for item in whole_graph.degree:\n",
    "        if item[1] == 0:\n",
    "            isolated_nodes.append(item[0])\n",
    "    whole_graph.remove_nodes_from(isolated_nodes)\n",
    "    print(\"remove {} isolated_nodes\".format(len(isolated_nodes)))\n",
    "    return whole_graph, isolated_nodes\n",
    "\n",
    "\n",
    "def count_distance_of_every_two_joint_nodes(whole_graph, userLocation, U_train, U_dev, U_test, edge_dis_file):\n",
    "    print('Nodes: %d, Edges: %d' % (nx.number_of_nodes(whole_graph), nx.number_of_edges(whole_graph)))\n",
    "    index_to_userId = list(U_train + U_dev + U_test)\n",
    "    edge_list = list(whole_graph.edges)\n",
    "    dis_list = list()  # store the edge distance of the whole graph\n",
    "    # dis_data = ([0, 3], 5)        # distance between user_id:0 and user_id:3 is 5,  5 = dis<0, 3>\n",
    "    A_index = 0\n",
    "    for item in edge_list:\n",
    "        # add the nodes which has no edge\n",
    "        if item[0] - A_index > 1:\n",
    "            no_edge_index = range(A_index + 1, item[0])\n",
    "            for temp in no_edge_index:\n",
    "                use_id = index_to_userId[temp]\n",
    "                dis_data = ([use_id, use_id], 0)\n",
    "                dis_list.append(dis_data)\n",
    "\n",
    "        # get the index of nodes in the whole graph\n",
    "        A_index = item[0]\n",
    "        B_index = item[1]\n",
    "        # get user_id\n",
    "        use_id_A = index_to_userId[A_index]\n",
    "        use_id_B = index_to_userId[B_index]\n",
    "        # get the location of user using user_id\n",
    "        locationA = userLocation[use_id_A].split(',')\n",
    "        locationB = userLocation[use_id_B].split(',')\n",
    "        # calculate the distance\n",
    "        distance = haversine((float(locationA[0]), float(locationA[1])),\n",
    "                             (float(locationB[0]), float(locationB[1])))\n",
    "        dis_data = ([use_id_A, use_id_B], distance)\n",
    "        dis_list.append(dis_data)\n",
    "\n",
    "    # add extra edge=0 for some large_index nodes which has no edge\n",
    "    largest_index = edge_list[-1][0]\n",
    "    node_num = len(whole_graph.nodes)\n",
    "    if largest_index < node_num:\n",
    "        no_edge_index = range(largest_index + 1, node_num)\n",
    "        for temp in no_edge_index:\n",
    "            use_id = index_to_userId[temp]\n",
    "            dis_data = ([use_id, use_id], 0)\n",
    "            dis_list.append(dis_data)\n",
    "\n",
    "    # store the dump file\n",
    "    dump_obj(dis_list, edge_dis_file)\n",
    "    print(\"dis_list data format: [(<use_id_A, use_id_B>, dis), (<use_id_A, use_id_C>, dis), ...]\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "def count_distance_for_train_data(whole_graph, userLocation, U_train, edge_dis_file):\n",
    "    print('Nodes: %d, Edges: %d' % (nx.number_of_nodes(whole_graph), nx.number_of_edges(whole_graph)))\n",
    "    index_to_userId = list(U_train)  # count the edges' information only for train userID\n",
    "    edge_list_old = list(whole_graph.edges)\n",
    "\n",
    "    '''only select train nodes and reproduce new edge_list'''\n",
    "    edge_list = list()\n",
    "    train_len = len(index_to_userId)\n",
    "    for item in edge_list_old:\n",
    "        if (item[0] < train_len) and (item[1] < train_len):\n",
    "            edge_list.append(item)\n",
    "        else:\n",
    "            continue\n",
    "    print(\"len of edge_list is:{}\".format(len(edge_list)))\n",
    "\n",
    "    dis_list = list()  # store the edge distance of the whole graph\n",
    "    A_index = 0\n",
    "    for item in edge_list:\n",
    "        # add the nodes which has no edge\n",
    "        if item[0] - A_index > 1:\n",
    "            no_edge_index = range(A_index + 1, item[0])\n",
    "            for temp in no_edge_index:\n",
    "                use_id = index_to_userId[temp]\n",
    "                dis_data = ([use_id, use_id], 0)\n",
    "                dis_list.append(dis_data)\n",
    "\n",
    "        # get the index of nodes in the whole graph\n",
    "        A_index = item[0]\n",
    "        B_index = item[1]\n",
    "        # get user_id\n",
    "        use_id_A = index_to_userId[A_index]\n",
    "        use_id_B = index_to_userId[B_index]\n",
    "        # get the location of user using user_id\n",
    "        locationA = userLocation[use_id_A].split(',')\n",
    "        locationB = userLocation[use_id_B].split(',')\n",
    "        # calculate the distance\n",
    "        distance = haversine((float(locationA[0]), float(locationA[1])),\n",
    "                             (float(locationB[0]), float(locationB[1])))\n",
    "        dis_data = ([use_id_A, use_id_B], distance)\n",
    "        dis_list.append(dis_data)\n",
    "\n",
    "    # add extra edge=0 for some large_index nodes which has no edge\n",
    "    largest_index = edge_list[-1][0]\n",
    "    node_num = len(index_to_userId)\n",
    "    if largest_index < node_num:\n",
    "        no_edge_index = range(largest_index + 1, node_num)\n",
    "        for temp in no_edge_index:\n",
    "            use_id = index_to_userId[temp]\n",
    "            dis_data = ([use_id, use_id], 0)\n",
    "            dis_list.append(dis_data)\n",
    "\n",
    "    # store the dump file\n",
    "    dump_obj(dis_list, edge_dis_file)\n",
    "    print(\"dis_list data format: [(<use_id_A, use_id_B>, dis), (<use_id_A, use_id_C>, dis), ...]\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "def analyze_distance(edge_dis_file='./my_assets/no_ues_now/edge_dis.pkl'):\n",
    "    # load dis_list from file\n",
    "    if os.path.exists(edge_dis_file):\n",
    "        print('loading dis_list from {}'.format(edge_dis_file))\n",
    "        dis_list = load_obj(edge_dis_file)\n",
    "    else:\n",
    "        exit(\"there is no {}\".format(edge_dis_file))\n",
    "    print(\"len(dis_list):{}\".format(len(dis_list)))\n",
    "    # gather all distance of every user_id\n",
    "    user_dis = list()  # data format: [(user_id_A, disx, disy, disz), (user_id_B, 0), ...]\n",
    "    one_user = list()  # data format: [user_id_A, disx, disy, disz]\n",
    "    user_id_A = dis_list[0][0][0]\n",
    "\n",
    "    for index, item in enumerate(dis_list):\n",
    "        # judge whether is the same user with the last one.\n",
    "        if item[0][0] != user_id_A:\n",
    "            user_dis.append(one_user)\n",
    "            one_user = list()\n",
    "            user_id_A = item[0][0]\n",
    "\n",
    "        # store only a user_id\n",
    "        if len(one_user) == 0:\n",
    "            one_user.append(item[0][0])  # add user_id\n",
    "        one_user.append(item[1])  # add dis\n",
    "\n",
    "        # store the last user_id information of dis_list\n",
    "        if index == len(dis_list) - 1:\n",
    "            user_dis.append(one_user)\n",
    "    print(\"len(user_dis):{}\".format(len(user_dis)))\n",
    "\n",
    "    # count the average edge distance of every user_id\n",
    "    ave_list = list()\n",
    "    all_dis_list = list()  # in order to get the mean and variance of all edges.\n",
    "    for item in user_dis:\n",
    "        ave_list.append(np.mean(item[1:]))\n",
    "        for edge_dis_temp in item[1:]:  # in order to get the mean and variance of all edges.\n",
    "            all_dis_list.append(edge_dis_temp)  # in order to get the mean and variance of all edges.\n",
    "    print(\"the average distance of all edges:{}\".format(np.mean(ave_list)))\n",
    "    print(\"len of all_dis_list:{}\\t mean:{}\\t variance:{}\"\n",
    "          .format(len(all_dis_list), np.mean(all_dis_list), np.var(all_dis_list)))\n",
    "\n",
    "    # count the distance distribution of user's average dis\n",
    "    dis_distribution = [0] * 7  # distribution list\n",
    "    for item in ave_list:\n",
    "        index = int(math.ceil(item / 1000))  # math.ceil(2.3)-->3\n",
    "        if index > 6:\n",
    "            dis_distribution[6] += 1\n",
    "        else:\n",
    "            dis_distribution[index] += 1\n",
    "\n",
    "    # count the distance distribution of the whole edges\n",
    "    dis_distri_all = [0] * 7  # distribution list\tgap 1000\n",
    "    for item in dis_list:\n",
    "        index = int(math.ceil(item[1] / 1000))  # math.ceil(2.3)-->3\n",
    "        if index > 6:\n",
    "            dis_distri_all[6] += 1\n",
    "        else:\n",
    "            dis_distri_all[index] += 1\n",
    "\n",
    "    # count the distance distribution of the whole edges\n",
    "    dis_distri_all2 = [0] * 11  # distribution list gap 500\n",
    "    for item in dis_list:\n",
    "        index = int(math.ceil(item[1] / 500))  # math.ceil(2.3)-->3\n",
    "        if index > 10:\n",
    "            dis_distri_all2[10] += 1\n",
    "        else:\n",
    "            dis_distri_all2[index] += 1\n",
    "\n",
    "    # draw the picture\n",
    "    x_data_1 = range(0, len(ave_list))\n",
    "    y_data_1 = ave_list\n",
    "    x_data_2 = range(0, 7)\n",
    "    y_data_2 = dis_distribution\n",
    "    x_data_3 = range(0, 7)\n",
    "    y_data_3 = dis_distri_all\n",
    "    x_data_4 = range(0, 11)\n",
    "    y_data_4 = dis_distri_all2\n",
    "    name_list = ['0', '(0,1K]', '(1K,2K]', '(2K,3K]', '(3K,4K]', '(4K,5K]', '(5K,++)']\n",
    "    name_list2 = ['0', '(0,0.5]', '(0.5,1]', '(1,1.5]', '(1.5,2]', '(2,2.5]', '(2.5,3]',\n",
    "                  '(3,3.5]', '(3.5,4]', '(4,4.5]', '(4.5,++)']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(x_data_1, y_data_1, label='average distance', color='b', linestyle='solid', linewidth=1)\n",
    "    plt.legend()\n",
    "    plt.xlabel('nodes_id')\n",
    "    plt.ylabel('aver_dis')\n",
    "    plt.title(\"the average distance of every nodes' edges.\")\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(x_data_2, y_data_2, label='dis_distribution', width=0.35, fc='r', tick_label=name_list)\n",
    "    plt.legend()\n",
    "    plt.xlabel('dis_distribution')\n",
    "    plt.ylabel('number')\n",
    "    plt.title(\"the distance distribution of nodes' average edges.\")\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(x_data_4, y_data_4, label='dis_distri_all', width=0.35, fc='y', tick_label=name_list2)\n",
    "    plt.legend()\n",
    "    plt.xlabel('dis_distribution(Unit:K)')\n",
    "    plt.ylabel('number')\n",
    "    plt.title(\"the distance distribution of the whole edges in the graph(gap 500).\")\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.bar(x_data_3, y_data_3, label='dis_distri_all', width=0.35, fc='y', tick_label=name_list)\n",
    "    plt.legend()\n",
    "    plt.xlabel('dis_distribution')\n",
    "    plt.ylabel('number')\n",
    "    plt.title(\"the distance distribution of the whole edges in the graph(gap 1000).\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return np.mean(all_dis_list), np.var(all_dis_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7963ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:07.421866Z",
     "iopub.status.busy": "2024-09-10T16:39:07.421404Z",
     "iopub.status.idle": "2024-09-10T16:39:07.515371Z",
     "shell.execute_reply": "2024-09-10T16:39:07.514189Z"
    },
    "papermill": {
     "duration": 0.11331,
     "end_time": "2024-09-10T16:39:07.518063",
     "exception": false,
     "start_time": "2024-09-10T16:39:07.404753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, data_home, bucket_size=50, encoding='utf-8', celebrity_threshold=10, one_hot_labels=False,\n",
    "                 mindf=10, maxdf=0.2, norm='l2', idf=True, btf=True, tokenizer=None, subtf=False, stops=None,\n",
    "                 token_pattern=r'(?u)(?<![#@])\\b\\w\\w+\\b', vocab=None):\n",
    "        self.data_home = data_home\n",
    "        self.bucket_size = bucket_size\n",
    "        self.encoding = encoding\n",
    "        self.celebrity_threshold = celebrity_threshold\n",
    "        self.one_hot_labels = one_hot_labels\n",
    "        self.mindf = mindf\n",
    "        self.maxdf = maxdf\n",
    "        self.norm = norm\n",
    "        self.idf = idf\n",
    "        self.btf = btf\n",
    "        self.tokenizer = tokenizer\n",
    "        self.subtf = subtf\n",
    "        self.stops = stops if stops else 'english'\n",
    "        self.token_pattern = r'(?u)(?<![#@|,.-_+^……$%&*(); :`，。？、：；;《》{}“”~#￥])\\b\\w\\w+\\b'\n",
    "        self.vocab = vocab\n",
    "        # self.biggraph = None\n",
    "\n",
    "    def load_data(self):\n",
    "        print('loading the dataset from: {}'.format(self.data_home))\n",
    "        train_file = os.path.join(self.data_home, 'user_info.train.txt')\n",
    "        dev_file = os.path.join(self.data_home, 'user_info.dev.txt')\n",
    "        test_file = os.path.join(self.data_home, 'user_info.test.txt')\n",
    "\n",
    "        df_train = pd.read_csv(train_file, delimiter='\\t', encoding=self.encoding, names=['user', 'lat', 'lon', 'text'],\n",
    "                               quoting=csv.QUOTE_NONE, on_bad_lines=\"skip\")\n",
    "        df_dev = pd.read_csv(dev_file, delimiter='\\t', encoding=self.encoding, names=['user', 'lat', 'lon', 'text'],\n",
    "                             quoting=csv.QUOTE_NONE, on_bad_lines=\"skip\")\n",
    "        df_test = pd.read_csv(test_file, delimiter='\\t', encoding=self.encoding, names=['user', 'lat', 'lon', 'text'],\n",
    "                              quoting=csv.QUOTE_NONE, on_bad_lines=\"skip\")\n",
    "        df_train.dropna(inplace=True)\n",
    "        df_dev.dropna(inplace=True)\n",
    "        df_test.dropna(inplace=True)\n",
    "\n",
    "        df_train['user'] = df_train['user'].apply(lambda x: str(x).lower())\n",
    "        df_train.drop_duplicates(['user'], inplace=True, keep='last')\n",
    "        df_train.set_index(['user'], drop=True, append=False, inplace=True)\n",
    "        df_train.sort_index(inplace=True)\n",
    "\n",
    "        df_dev['user'] = df_dev['user'].apply(lambda x: str(x).lower())\n",
    "        df_dev.drop_duplicates(['user'], inplace=True, keep='last')\n",
    "        df_dev.set_index(['user'], drop=True, append=False, inplace=True)\n",
    "        df_dev.sort_index(inplace=True)\n",
    "\n",
    "        df_test['user'] = df_test['user'].apply(lambda x: str(x).lower())\n",
    "        df_test.drop_duplicates(['user'], inplace=True, keep='last')\n",
    "        df_test.set_index(['user'], drop=True, append=False, inplace=True)\n",
    "        df_test.sort_index(inplace=True)\n",
    "\n",
    "        self.df_train = df_train\n",
    "        self.df_dev = df_dev\n",
    "        self.df_test = df_test\n",
    "\n",
    "    def get_graph(self):\n",
    "        g = nx.Graph()\n",
    "        # 'user'\n",
    "        nodes = set(self.df_train.index.tolist() + self.df_dev.index.tolist() + self.df_test.index.tolist())\n",
    "        assert len(nodes) == len(self.df_train) + len(self.df_dev) + len(self.df_test), 'duplicate target node'\n",
    "        nodes_list = self.df_train.index.tolist() + self.df_dev.index.tolist() + self.df_test.index.tolist()\n",
    "        node_id = {node: id for id, node in enumerate(nodes_list)}\n",
    "        g.add_nodes_from(node_id.values())\n",
    "        for node in nodes:\n",
    "            g.add_edge(node_id[node], node_id[node])\n",
    "        pattern = '(?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))@([A-Za-z]+[A-Za-z0-9_]+)'\n",
    "        pattern = re.compile(pattern)\n",
    "        print('start adding the train graph')\n",
    "        externalNum = 0\n",
    "        for i in range(len(self.df_train)):\n",
    "            user = self.df_train.index[i]\n",
    "            user_id = node_id[user]\n",
    "            mentions = [m.lower() for m in pattern.findall(self.df_train.text[i])]\n",
    "            idmentions = set()\n",
    "            for m in mentions:\n",
    "                if m in node_id:\n",
    "                    idmentions.add(node_id[m])\n",
    "                else:\n",
    "                    id = len(node_id)\n",
    "                    node_id[m] = id\n",
    "                    idmentions.add(id)\n",
    "                    externalNum += 1\n",
    "            if len(idmentions) > 0:\n",
    "                g.add_nodes_from(idmentions)\n",
    "            for id in idmentions:\n",
    "                g.add_edge(user_id, id)\n",
    "        print('start adding the dev graph')\n",
    "        externalNum = 0\n",
    "        for i in range(len(self.df_dev)):\n",
    "            user = self.df_dev.index[i]\n",
    "            user_id = node_id[user]\n",
    "            mentions = [m.lower() for m in pattern.findall(self.df_dev.text[i])]\n",
    "            idmentions = set()\n",
    "            for m in mentions:\n",
    "                if m in node_id:\n",
    "                    idmentions.add(node_id[m])\n",
    "                else:\n",
    "                    id = len(node_id)\n",
    "                    node_id[m] = id\n",
    "                    idmentions.add(id)\n",
    "                    externalNum += 1\n",
    "            if len(idmentions) > 0:\n",
    "                g.add_nodes_from(idmentions)\n",
    "            for id in idmentions:\n",
    "                g.add_edge(id, user_id)\n",
    "        print('start adding the test graph')\n",
    "        externalNum = 0\n",
    "        for i in range(len(self.df_test)):\n",
    "            user = self.df_test.index[i]\n",
    "            user_id = node_id[user]\n",
    "            mentions = [m.lower() for m in pattern.findall(self.df_test.text[i])]\n",
    "            idmentions = set()\n",
    "            for m in mentions:\n",
    "                if m in node_id:\n",
    "                    idmentions.add(node_id[m])\n",
    "                else:\n",
    "                    id = len(node_id)\n",
    "                    node_id[m] = id\n",
    "                    idmentions.add(id)\n",
    "                    externalNum += 1\n",
    "            if len(idmentions) > 0:\n",
    "                g.add_nodes_from(idmentions)\n",
    "            for id in idmentions:\n",
    "                g.add_edge(id, user_id)\n",
    "        print('#nodes: %d, #edges: %d' % (nx.number_of_nodes(g), nx.number_of_edges(g)))\n",
    "\n",
    "        celebrities = []\n",
    "        for i in range(len(nodes_list), len(node_id)):\n",
    "            deg = len(g[i])\n",
    "            if deg == 1 or deg > self.celebrity_threshold:\n",
    "                celebrities.append(i)\n",
    "\n",
    "        print('removing %d celebrity nodes with degree higher than %d' % (len(celebrities), self.celebrity_threshold))\n",
    "        g.remove_nodes_from(celebrities)\n",
    "        print('projecting the graph')\n",
    "        projected_g = efficient_collaboration_weighted_projected_graph2(g, range(len(nodes_list)))\n",
    "        print('#nodes: %d, #edges: %d' % (nx.number_of_nodes(projected_g), nx.number_of_edges(projected_g)))\n",
    "        self.graph = projected_g\n",
    "\n",
    "    def get_graph_temp(self):\n",
    "        g = nx.Graph()\n",
    "        nodes = set(self.df_train.index.tolist() + self.df_dev.index.tolist() + self.df_test.index.tolist())\n",
    "        assert len(nodes) == len(self.df_train) + len(self.df_dev) + len(self.df_test), 'duplicate target node'\n",
    "        nodes_list = self.df_train.index.tolist() + self.df_dev.index.tolist() + self.df_test.index.tolist()\n",
    "        node_id = {node: id for id, node in enumerate(nodes_list)}\n",
    "        g.add_nodes_from(node_id.values())\n",
    "        train_locs = self.df_train[['lat', 'lon']].values\n",
    "        for node in nodes:\n",
    "            g.add_edge(node_id[node], node_id[node])\n",
    "        pattern = '(?<=^|(?<=[^a-zA-Z0-9-_\\\\.]))@([A-Za-z]+[A-Za-z0-9_]+)'\n",
    "        pattern = re.compile(pattern)\n",
    "        print('adding the train graph')\n",
    "        for i in range(len(self.df_train)):\n",
    "            user = self.df_train.index[i]\n",
    "            user_id = node_id[user]\n",
    "            mentions = [m for m in pattern.findall(self.df_train.text[i])]\n",
    "            idmentions = set()\n",
    "            for m in mentions:\n",
    "                if m in node_id:\n",
    "                    idmentions.add(node_id[m])\n",
    "                else:\n",
    "                    id = len(node_id)\n",
    "                    node_id[m] = id\n",
    "                    idmentions.add(id)\n",
    "            if len(idmentions) > 0:\n",
    "                g.add_nodes_from(idmentions)\n",
    "            for id in idmentions:\n",
    "                g.add_edge(id, user_id)\n",
    "        celebrities = []\n",
    "        for i in range(len(nodes_list), len(node_id)):\n",
    "            deg = len(g[i])\n",
    "            if deg > self.celebrity_threshold:\n",
    "                celebrities.append(i)\n",
    "        # get neighbours of celebrities\n",
    "        id_node = {v: k for k, v in node_id.iteritems()}\n",
    "\n",
    "        degree_distmean = defaultdict(list)\n",
    "        degree_distance = defaultdict(list)\n",
    "        c_distmean = {}\n",
    "        for c in celebrities:\n",
    "            c_name = id_node[c]\n",
    "            c_nbrs = g[c].keys()\n",
    "            c_degree = len(c_nbrs)\n",
    "            c_locs = train_locs[c_nbrs, :]\n",
    "            c_lats = c_locs[:, 0]\n",
    "            c_lons = c_locs[:, 1]\n",
    "            c_median_lat = np.median(c_lats)\n",
    "            c_median_lon = np.median(c_lons)\n",
    "            distances = [haversine((c_median_lat, c_median_lon), tuple(c_locs[i].tolist())) for i in\n",
    "                         range(c_locs.shape[0])]\n",
    "            degree_distance[c_degree].extend(distances)\n",
    "            c_meandist = np.mean(distances)\n",
    "            degree_distmean[c_degree].append(c_meandist)\n",
    "            c_distmean[c_name] = [c_degree, c_meandist]\n",
    "        with open('celebrity.pkl', 'wb') as fin:\n",
    "            pickle.dump((c_distmean, degree_distmean, degree_distance), fin)\n",
    "\n",
    "        print('removing %d celebrity nodes with degree higher than %d' % (len(celebrities), self.celebrity_threshold))\n",
    "        self.biggraph = g\n",
    "\n",
    "    def longest_path(self, g):\n",
    "        nodes = g.nodes()\n",
    "        pathlen_counter = Counter()\n",
    "        for n1 in nodes:\n",
    "            for n2 in nodes:\n",
    "                if n1 < n2:\n",
    "                    for path in nx.all_simple_paths(g, source=n1, target=n2):\n",
    "                        pathlen = len(path)\n",
    "                        pathlen_counter[pathlen] += 1\n",
    "        return pathlen_counter\n",
    "\n",
    "    def list_dats_save_to_txt(self, file_path, data):  # file_path 为写入文件的路径，data为要写入数据列表.\n",
    "        file = open(file_path, 'a', encoding='utf-8')\n",
    "        for i in range(len(data)):\n",
    "            file.write(str(data[i]) + '\\n')\n",
    "        file.close()\n",
    "        print(\"{} save success !\".format(file_path))\n",
    "\n",
    "        # .replace('[', '').replace(']', '')  # 去除[],这两行按数据不同，可以选择\n",
    "        # \ts = s.replace(\"'\", '').replace(',', '') + '\\n'  # 去除单引号，逗号，每行末尾追加换行符\n",
    "\n",
    "    def doc2vec_feature(self, doc2vec_model_file):\n",
    "        # load model\n",
    "        model = gensim.models.doc2vec.Doc2Vec.load(doc2vec_model_file)\n",
    "\n",
    "        # train data features\n",
    "        feature_list = list()\n",
    "        index_l = 0\n",
    "        index_r = len(self.df_train.text)\n",
    "        for i in range(index_l, index_r):\n",
    "            feature_list.append(model.docvecs[i])\n",
    "        self.X_train = np.array(feature_list)\n",
    "\n",
    "        # dev data features\n",
    "        feature_list = list()\n",
    "        index_l = len(self.df_train.text)\n",
    "        index_r = len(self.df_train.text) + len(self.df_dev.text)\n",
    "        for i in range(index_l, index_r):\n",
    "            feature_list.append(model.docvecs[i])\n",
    "        self.X_dev = np.array(feature_list)\n",
    "\n",
    "        # test data features\n",
    "        feature_list = list()\n",
    "        index_l = len(self.df_train.text) + len(self.df_dev.text)\n",
    "        index_r = len(self.df_train.text) + len(self.df_dev.text) + len(self.df_test.text)\n",
    "        for i in range(index_l, index_r):\n",
    "            feature_list.append(model.docvecs[i])\n",
    "        self.X_test = np.array(feature_list)\n",
    "\n",
    "        print(\"training    n_samples: %d, n_features: %d\" % self.X_train.shape)\n",
    "        print(\"development n_samples: %d, n_features: %d\" % self.X_dev.shape)\n",
    "        print(\"test        n_samples: %d, n_features: %d\" % self.X_test.shape)\n",
    "\n",
    "    def tfidf(self):\n",
    "        # keep both hashtags and mentions\n",
    "        # token_pattern=r'(?u)@?#?\\b\\w\\w+\\b'\n",
    "        # remove hashtags and mentions\n",
    "        # token_pattern = r'(?u)(?<![#@])\\b\\w+\\b'\n",
    "        # just remove mentions and remove hashsign from hashtags\n",
    "        # token_pattern = r'(?u)(?<![@])\\b\\w+\\b'\n",
    "        # remove mentions but keep hashtags with their sign\n",
    "        # token_pattern = r'(?u)(?<![@])#?\\b\\w\\w+\\b'\n",
    "        # remove multple occurrences of a character after 2 times yesss => yess\n",
    "        # re.sub(r\"(.)\\1+\", r\"\\1\\1\", s)\n",
    "\n",
    "        \"\"\"\n",
    "        # Morton add for save the words data into the file.\n",
    "        self.list_dats_save_to_txt(\"./data/cmu/train_corpus/cmu_train.txt\", list(self.df_train.text.values))\n",
    "        self.list_dats_save_to_txt(\"./data/cmu/train_corpus/cmu_dev.txt\",  list(self.df_dev.text.values))\n",
    "        self.list_dats_save_to_txt(\"./data/cmu/train_corpus/cmu_test.txt\",  list(self.df_test.text.values))\n",
    "        \"\"\"\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(tokenizer=self.tokenizer, token_pattern=self.token_pattern, use_idf=self.idf,\n",
    "                                          norm=self.norm, binary=self.btf, sublinear_tf=self.subtf,\n",
    "                                          min_df=self.mindf, max_df=self.maxdf, ngram_range=(1, 1),\n",
    "                                          stop_words=self.stops,\n",
    "                                          vocabulary=self.vocab, encoding=self.encoding, dtype=np.float32)\n",
    "        self.X_train = self.vectorizer.fit_transform(self.df_train.text.values)\n",
    "        self.X_dev = self.vectorizer.transform(self.df_dev.text.values)\n",
    "        self.X_test = self.vectorizer.transform(self.df_test.text.values)\n",
    "        print(\"training    n_samples: %d, n_features: %d\" % self.X_train.shape)\n",
    "        print(\"development n_samples: %d, n_features: %d\" % self.X_dev.shape)\n",
    "        print(\"test        n_samples: %d, n_features: %d\" % self.X_test.shape)\n",
    "\n",
    "        ''' drop out, never used. in order to calculate the  sum(tfidf) * context_features '''\n",
    "        # X_train = self.vectorizer.fit_transform(self.df_train.text.values)\n",
    "        # X_dev = self.vectorizer.transform(self.df_dev.text.values)\n",
    "        # X_test = self.vectorizer.transform(self.df_test.text.values)\n",
    "        # tf_idf_sum = np.vstack((X_train.toarray(), X_dev.toarray(), X_test.toarray()))\n",
    "        # self.tf_idf_sum = np.sum(tf_idf_sum, axis=1)\n",
    "        # print(\"tf_idf_sum done.\")\n",
    "\n",
    "    def assignClasses(self):\n",
    "        clusterer = KDTreeClustering(bucket_size=self.bucket_size)\n",
    "        train_locs = self.df_train[['lat', 'lon']].values\n",
    "        clusterer.fit(train_locs)\n",
    "        clusters = clusterer.get_clusters()\n",
    "        cluster_points = defaultdict(list)\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            cluster_points[cluster].append(train_locs[i])\n",
    "        print('# the number of clusterer labels is: %d' % len(cluster_points))\n",
    "        self.cluster_median = OrderedDict()\n",
    "        for cluster in sorted(cluster_points):\n",
    "            points = cluster_points[cluster]\n",
    "            median_lat = np.median([p[0] for p in points])\n",
    "            median_lon = np.median([p[1] for p in points])\n",
    "            self.cluster_median[cluster] = (median_lat, median_lon)\n",
    "        dev_locs = self.df_dev[['lat', 'lon']].values\n",
    "        test_locs = self.df_test[['lat', 'lon']].values\n",
    "        nnbr = NearestNeighbors(n_neighbors=1, algorithm='brute', leaf_size=1, metric=haversine, n_jobs=4)\n",
    "        nnbr.fit(np.array(list(self.cluster_median.values())))\n",
    "        self.dev_classes = nnbr.kneighbors(dev_locs, n_neighbors=1, return_distance=False)[:, 0]\n",
    "        self.test_classes = nnbr.kneighbors(test_locs, n_neighbors=1, return_distance=False)[:, 0]\n",
    "\n",
    "        self.train_classes = clusters\n",
    "\n",
    "        if self.one_hot_labels:\n",
    "            num_labels = np.max(self.train_classes) + 1\n",
    "            y_train = np.zeros((len(self.train_classes), num_labels), dtype=np.float32)\n",
    "            y_train[np.arange(len(self.train_classes)), self.train_classes] = 1\n",
    "            y_dev = np.zeros((len(self.dev_classes), num_labels), dtype=np.float32)\n",
    "            y_dev[np.arange(len(self.dev_classes)), self.dev_classes] = 1\n",
    "            y_test = np.zeros((len(self.test_classes), num_labels), dtype=np.float32)\n",
    "            y_test[np.arange(len(self.test_classes)), self.test_classes] = 1\n",
    "            self.train_classes = y_train\n",
    "            self.dev_classes = y_dev\n",
    "            self.test_classes = y_test\n",
    "\n",
    "    def encodingContent(self, vacab_size=80000, encod_size=500, padding=0):\n",
    "        vocab = dict()\n",
    "        total_words = list()\n",
    "        ignore = [' ', '|||', 'RT']\n",
    "\n",
    "        # get_vocab_and_total_words from content\n",
    "        def get_vocab_and_total_words(all_content):\n",
    "            for line in all_content:\n",
    "                # get words list\n",
    "                line = line.encode(\"utf-8\")\n",
    "                words = line.split()\n",
    "                # remove the ignore words\n",
    "                for ign in ignore:\n",
    "                    condition = lambda t: t != ign\n",
    "                    words = list(filter(condition, words))\n",
    "                # save all words\n",
    "                total_words.append(words)\n",
    "                # count words and number in vocab dict\n",
    "                for word in words:\n",
    "                    if vocab.has_key(word):\n",
    "                        vocab[word] += 1\n",
    "                    else:\n",
    "                        vocab[word] = 1\n",
    "\n",
    "        get_vocab_and_total_words(self.df_train.text.values)\n",
    "        get_vocab_and_total_words(self.df_dev.text.values)\n",
    "        get_vocab_and_total_words(self.df_test.text.values)\n",
    "\n",
    "        # sort the vocab 'key' according to the 'value'\n",
    "        vocab_list = sorted(vocab.items(), key=lambda d: d[1], reverse=True)\n",
    "        vocab_list = vocab_list[0:vacab_size]\n",
    "        vocab_new = list()\n",
    "        for i in vocab_list:\n",
    "            vocab_new.append(i[0])\n",
    "\n",
    "        # show information of content\n",
    "        words_count = 0\n",
    "        for line in total_words:\n",
    "            words_count += len(line)\n",
    "        print(\"the number of total words:{}\\nline number:{}\\naverage of line:{}\"\n",
    "              .format(words_count, len(total_words), words_count / len(total_words)))\n",
    "\n",
    "        # # normalization according to row, each row represent a feature\n",
    "        # def feature_normalization1(dt):\n",
    "        # \tmean_num = np.mean(dt, axis=0)\n",
    "        # \tsigma = np.std(dt, axis=0)\n",
    "        # \treturn (dt-mean_num)/sigma\n",
    "        #\n",
    "        # # normalization according to row, each row represent a feature\n",
    "        # def feature_normalization2(dt):\n",
    "        # \tmean_num = np.mean(dt, axis=0)\n",
    "        # \tmax_num = np.max(dt, axis=0)\n",
    "        # \tmin_num = np.min(dt, axis=0)\n",
    "        # \treturn (dt-mean_num)/(max_num-min_num)\n",
    "\n",
    "        # get_index_form_vocab, all_words:train, dev, test\n",
    "        def get_index_form_vocab(all_words):\n",
    "            print(\"start get index...\")\n",
    "            encoding_words = list()\n",
    "            for line in all_words:\n",
    "                line_list = [0] * encod_size\n",
    "                # cut line down to encod_size or padding\n",
    "                if len(line) >= encod_size:\n",
    "                    line = line[0:encod_size]\n",
    "                # get the true index\n",
    "                for i, word in enumerate(line):\n",
    "                    if word in vocab_new:\n",
    "                        line_list[i] = vocab_new.index(word)\n",
    "                encoding_words.append(line_list)\n",
    "            return np.array(encoding_words, dtype=float)\n",
    "\n",
    "        train_words = total_words[0:self.df_train.text.shape[0]]\n",
    "        dev_words = total_words[self.df_train.text.shape[0]:self.df_train.text.shape[0] + self.df_dev.text.shape[0]]\n",
    "        test_words = total_words[self.df_train.text.shape[0] + self.df_dev.text.shape[0]:]\n",
    "        self.X_train = get_index_form_vocab(train_words)\n",
    "        self.X_dev = get_index_form_vocab(dev_words)\n",
    "        self.X_test = get_index_form_vocab(test_words)\n",
    "        print(\"training    n_samples: %d, n_features: %d\" % self.X_train.shape)\n",
    "        print(\"development n_samples: %d, n_features: %d\" % self.X_dev.shape)\n",
    "        print(\"test        n_samples: %d, n_features: %d\" % self.X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8f0509f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:07.549556Z",
     "iopub.status.busy": "2024-09-10T16:39:07.549115Z",
     "iopub.status.idle": "2024-09-10T16:39:07.557154Z",
     "shell.execute_reply": "2024-09-10T16:39:07.555982Z"
    },
    "papermill": {
     "duration": 0.02708,
     "end_time": "2024-09-10T16:39:07.559701",
     "exception": false,
     "start_time": "2024-09-10T16:39:07.532621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataLoader at 0x7aed79e5d6f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(data_home='/kaggle/input/geotext', bucket_size=50, encoding='latin1', celebrity_threshold=5, mindf=10,\n",
    "                    token_pattern=r'(?u)(?<![@])#?\\b\\w\\w+\\b')\n",
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b95398b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:07.591394Z",
     "iopub.status.busy": "2024-09-10T16:39:07.590567Z",
     "iopub.status.idle": "2024-09-10T16:39:08.712806Z",
     "shell.execute_reply": "2024-09-10T16:39:08.711537Z"
    },
    "papermill": {
     "duration": 1.141032,
     "end_time": "2024-09-10T16:39:08.715496",
     "exception": false,
     "start_time": "2024-09-10T16:39:07.574464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the dataset from: /kaggle/input/geotext\n"
     ]
    }
   ],
   "source": [
    "dl.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6de12ac9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:08.748858Z",
     "iopub.status.busy": "2024-09-10T16:39:08.748401Z",
     "iopub.status.idle": "2024-09-10T16:39:16.305837Z",
     "shell.execute_reply": "2024-09-10T16:39:16.304293Z"
    },
    "papermill": {
     "duration": 7.577781,
     "end_time": "2024-09-10T16:39:16.308474",
     "exception": false,
     "start_time": "2024-09-10T16:39:08.730693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start adding the train graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/691640839.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mentions = [m.lower() for m in pattern.findall(self.df_train.text[i])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start adding the dev graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/691640839.py:93: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mentions = [m.lower() for m in pattern.findall(self.df_dev.text[i])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start adding the test graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/691640839.py:112: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mentions = [m.lower() for m in pattern.findall(self.df_test.text[i])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nodes: 128269, #edges: 193185\n",
      "removing 91477 celebrity nodes with degree higher than 5\n",
      "projecting the graph\n",
      "#nodes: 9475, #edges: 77155\n"
     ]
    }
   ],
   "source": [
    "dl.get_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b0bfcb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:16.342406Z",
     "iopub.status.busy": "2024-09-10T16:39:16.341852Z",
     "iopub.status.idle": "2024-09-10T16:39:22.012634Z",
     "shell.execute_reply": "2024-09-10T16:39:22.011397Z"
    },
    "papermill": {
     "duration": 5.691606,
     "end_time": "2024-09-10T16:39:22.016112",
     "exception": false,
     "start_time": "2024-09-10T16:39:16.324506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# the number of clusterer labels is: 129\n"
     ]
    }
   ],
   "source": [
    "dl.assignClasses()  # create the label (129 for cmu dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8af83214",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:22.057178Z",
     "iopub.status.busy": "2024-09-10T16:39:22.056609Z",
     "iopub.status.idle": "2024-09-10T16:39:25.375944Z",
     "shell.execute_reply": "2024-09-10T16:39:25.374303Z"
    },
    "papermill": {
     "duration": 3.341324,
     "end_time": "2024-09-10T16:39:25.379116",
     "exception": false,
     "start_time": "2024-09-10T16:39:22.037792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training    n_samples: 5685, n_features: 216\n",
      "development n_samples: 1895, n_features: 216\n",
      "test        n_samples: 1895, n_features: 216\n"
     ]
    }
   ],
   "source": [
    "dl.tfidf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f48f3",
   "metadata": {
    "papermill": {
     "duration": 0.015529,
     "end_time": "2024-09-10T16:39:25.411771",
     "exception": false,
     "start_time": "2024-09-10T16:39:25.396242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "PlotFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1cd92ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:25.447012Z",
     "iopub.status.busy": "2024-09-10T16:39:25.446537Z",
     "iopub.status.idle": "2024-09-10T16:39:25.462047Z",
     "shell.execute_reply": "2024-09-10T16:39:25.460958Z"
    },
    "papermill": {
     "duration": 0.037248,
     "end_time": "2024-09-10T16:39:25.465078",
     "exception": false,
     "start_time": "2024-09-10T16:39:25.427830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def draw_representations(X, y, k=4, seed=77, do_pca=True, filename='./Pics/output.png'):\n",
    "    \"\"\"\n",
    "    :param X: features with high dimension.\n",
    "    :param y: labels corresponding to the features, also called class.\n",
    "    :param k: the kinds of class to show.\n",
    "    :param seed: the random seed.\n",
    "    :param do_pca: if true, use PCA to decrease the dimension.\n",
    "    :param filename: the path and format(e.g. .png .pdf) to save the result.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "    import numpy as np\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn.decomposition import PCA\n",
    "    import scipy as sp\n",
    "\n",
    "    print(\"start draw_representations ...\")\n",
    "    class_count = Counter(y.tolist()).most_common(k)\n",
    "    num_samples = class_count[3][1] - class_count[3][1] % 10\n",
    "    all_lbls = []\n",
    "    all_samples = []\n",
    "    for i, cc in enumerate(class_count):\n",
    "        lbl, _ = cc\n",
    "        samples = X[y == lbl][0:num_samples, :]\n",
    "        samples = samples.todense() if sp.sparse.issparse(samples) else samples\n",
    "        lbls = y[y == lbl][0:num_samples]\n",
    "        lbls[:] = i\n",
    "        all_samples.append(samples)\n",
    "        all_lbls.append(lbls)\n",
    "    all_lbls = np.hstack(all_lbls)\n",
    "    all_samples = np.vstack(all_samples)\n",
    "    if do_pca:\n",
    "        pca = PCA(n_components=50, random_state=seed)\n",
    "        all_samples = pca.fit_transform(all_samples)\n",
    "    tsne = TSNE(n_components=2, random_state=seed)\n",
    "    embeddings = tsne.fit_transform(all_samples)\n",
    "    chosen_indices = np.random.choice(np.arange(embeddings.shape[0]), size=k * min(50, num_samples), replace=False)\n",
    "    chosen_embeddings = embeddings[chosen_indices, :]\n",
    "    chosen_ys = all_lbls[chosen_indices]\n",
    "\n",
    "    plt.axis('off')\n",
    "    # plt.title(\"samples: {}    class: {}\".format(min(50, num_samples), k))\n",
    "    # plt.scatter(chosen_embeddings[:, 0], chosen_embeddings[:, 1], chosen_embeddings[:, 2],\n",
    "    #             c=chosen_ys, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(chosen_embeddings[:, 0], chosen_embeddings[:, 1], c=chosen_ys, cmap=plt.cm.get_cmap(\"Set1\", k))\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263fcca3",
   "metadata": {
    "papermill": {
     "duration": 0.015514,
     "end_time": "2024-09-10T16:39:25.497711",
     "exception": false,
     "start_time": "2024-09-10T16:39:25.482197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## geoMain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "058e64c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:25.531185Z",
     "iopub.status.busy": "2024-09-10T16:39:25.530774Z",
     "iopub.status.idle": "2024-09-10T16:39:25.538563Z",
     "shell.execute_reply": "2024-09-10T16:39:25.536987Z"
    },
    "papermill": {
     "duration": 0.027956,
     "end_time": "2024-09-10T16:39:25.541533",
     "exception": false,
     "start_time": "2024-09-10T16:39:25.513577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from haversine import haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b3e0efd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:25.577289Z",
     "iopub.status.busy": "2024-09-10T16:39:25.576832Z",
     "iopub.status.idle": "2024-09-10T16:39:25.641291Z",
     "shell.execute_reply": "2024-09-10T16:39:25.639997Z"
    },
    "papermill": {
     "duration": 0.085545,
     "end_time": "2024-09-10T16:39:25.644320",
     "exception": false,
     "start_time": "2024-09-10T16:39:25.558775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_regression(model, train_features, train_labels, val_features, U_dev, classLatMedian, classLonMedian,\n",
    "                     userLocation, epochs=100, weight_decay=5e-6, lr=0.01, patience=10, model_file='myModel.pkl'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    train_patience = 0\n",
    "    val_acc_best = -1\n",
    "    epoch_best = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_features)\n",
    "        loss_train = F.cross_entropy(output, train_labels)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        '''verification, no gradient descent'''\n",
    "        _, _, val_acc, _, _, _ = geo_eval(model, val_features, U_dev, classLatMedian, classLonMedian, userLocation)\n",
    "\n",
    "        '''show val_acc,val_acc_best every 50 epoch'''\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"epoch:{}\\t \\tval_acc:{}\\t \\tval_acc_best:{}\".format(epoch, val_acc, val_acc_best))\n",
    "\n",
    "        '''apply early stop using val_acc_best, and save model'''\n",
    "        if val_acc > val_acc_best:\n",
    "            val_acc_best = val_acc\n",
    "            epoch_best = epoch\n",
    "            train_patience = 0\n",
    "            torch.save(model.state_dict(), model_file)\n",
    "        else:\n",
    "            train_patience += 1\n",
    "        if train_patience == patience:\n",
    "            print(\"Early stop! \\t epoch_best:{}\\t \\t \\tval_acc_best:{}\".format(epoch_best, val_acc_best))\n",
    "            break\n",
    "    return val_acc_best, epoch_best\n",
    "\n",
    "\n",
    "def train_regression2(model, train_features, train_labels, val_features, U_dev, classLatMedian, classLonMedian,\n",
    "                      userLocation, epochs=100, weight_decay=5e-6, lr=0.01, patience=10, model_file='myModel.pkl',\n",
    "                      cluster_nodes=None, cluster_adj=None, node2cluster_arr=None):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    train_patience = 0\n",
    "    val_acc_best = -1\n",
    "    epoch_best = 0\n",
    "    train_len = train_features.shape[0]\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_features, node2cluster_arr[0:train_len], cluster_nodes, cluster_adj)\n",
    "        loss_train = F.cross_entropy(output, train_labels)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        '''verification, no gradient descent'''\n",
    "        _, _, val_acc, _, _, _ = geo_eval2(model, val_features, U_dev, classLatMedian, classLonMedian,\n",
    "                                           userLocation, node2cluster_arr[train_len:train_len + val_features.shape[0]])\n",
    "\n",
    "        '''show val_acc,val_acc_best every 50 epoch'''\n",
    "        if epoch % 50 == 0:\n",
    "            print(\"epoch:{}\\t \\tval_acc:{}\\t \\tval_acc_best:{}\".format(epoch, val_acc, val_acc_best))\n",
    "\n",
    "        '''apply early stop using val_acc_best, and save model'''\n",
    "        if val_acc > val_acc_best:\n",
    "            val_acc_best = val_acc\n",
    "            epoch_best = epoch\n",
    "            train_patience = 0\n",
    "            # torch.save(model.state_dict(), model_file)\n",
    "            torch.save(model, model_file)\n",
    "        else:\n",
    "            train_patience += 1\n",
    "        if train_patience == patience:\n",
    "            print(\"Early stop! \\t epoch_best:{}\\t \\t \\tval_acc_best:{}\".format(epoch_best, val_acc_best))\n",
    "            break\n",
    "    return val_acc_best, epoch_best\n",
    "\n",
    "\n",
    "def geo_eval(model, features, U_test, classLatMedian, classLonMedian, userLocation):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        y_pred = model(features)\n",
    "    y_pred = y_pred.data.cpu().numpy()\n",
    "    y_pred = np.argmax(y_pred, axis=1)  # 1代表行\n",
    "\n",
    "    assert len(y_pred) == len(U_test), \"#preds: %d, #users: %d\" % (len(y_pred), len(U_test))\n",
    "\n",
    "    distances = []\n",
    "    latlon_pred = []\n",
    "    latlon_true = []\n",
    "    for i in range(0, len(y_pred)):\n",
    "        user = U_test[i]\n",
    "        location = userLocation[user].split(',')\n",
    "        lat, lon = float(location[0]), float(location[1])\n",
    "        latlon_true.append([lat, lon])\n",
    "        prediction = str(y_pred[i])\n",
    "        lat_pred, lon_pred = classLatMedian[prediction], classLonMedian[prediction]\n",
    "        latlon_pred.append([lat_pred, lon_pred, y_pred[i]])\n",
    "        distance = haversine((lat, lon), (lat_pred, lon_pred))\n",
    "        distances.append(distance)\n",
    "\n",
    "    acc_at_161 = 100 * len([d for d in distances if d < 161]) / float(len(distances))\n",
    "    return np.mean(distances), np.median(distances), acc_at_161, distances, latlon_true, latlon_pred\n",
    "\n",
    "\n",
    "def geo_eval2(model, features, U_test, classLatMedian, classLonMedian, userLocation, node2cluster_arr):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        y_pred = model(features, node2cluster_arr)\n",
    "    y_pred = y_pred.data.cpu().numpy()\n",
    "    y_pred = np.argmax(y_pred, axis=1)  # 1代表行\n",
    "\n",
    "    assert len(y_pred) == len(U_test), \"#preds: %d, #users: %d\" % (len(y_pred), len(U_test))\n",
    "\n",
    "    distances = []\n",
    "    latlon_pred = []\n",
    "    latlon_true = []\n",
    "    for i in range(0, len(y_pred)):\n",
    "        user = U_test[i]\n",
    "        location = userLocation[user].split(',')\n",
    "        lat, lon = float(location[0]), float(location[1])\n",
    "        latlon_true.append([lat, lon])\n",
    "        prediction = str(y_pred[i])\n",
    "        lat_pred, lon_pred = classLatMedian[prediction], classLonMedian[prediction]\n",
    "        latlon_pred.append([lat_pred, lon_pred, y_pred[i]])\n",
    "        distance = haversine((lat, lon), (lat_pred, lon_pred))\n",
    "        distances.append(distance)\n",
    "\n",
    "    acc_at_161 = 100 * len([d for d in distances if d < 161]) / float(len(distances))\n",
    "    return np.mean(distances), np.median(distances), acc_at_161, distances, latlon_true, latlon_pred\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    preprocess_data() :     load data from dataset and precess data into numpy format\n",
    "    process_data() :        port the data to pyTorch and convert to cuda\n",
    "    U_train, U_dev, U_test, classLatMedian, classLonMedian, userLocation : only use when Valid and Test\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"----preprocess_data\")\n",
    "    data = preprocess_data(args)\n",
    "    print(\"----process_data\")\n",
    "    data = process_data(data, args)\n",
    "    print(\"----Multiple assignement\")\n",
    "    (adj, features, labels, idx_train, idx_val, idx_test, U_train, U_dev, U_test,\n",
    "     classLatMedian, classLonMedian, userLocation, cluster_nodes, cluster_adj, node2cluster_arr) = data\n",
    "\n",
    "    \"\"\"\n",
    "    get model and train\n",
    "    input_dim: features.size(1)             # equal 9467 for ./data/cmu\n",
    "    output_dim: labels.max().item() + 1     # equal 129 for ./data/cmu\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"----get_model\")\n",
    "    model = get_model(args.model, features.shape[1], labels.max().item() + 1, usecuda=args.usecuda)\n",
    "    print(\"----model_file\")\n",
    "    model_file = \"/kaggle/working/{}_lr_{}.pkl\".format(args.model, str(args.lr))\n",
    "\n",
    "    if args.model == \"SGC\":\n",
    "        print(\"---- model == SGC\")\n",
    "        if args.vis == \"vis\":\n",
    "            #from plotFunc import draw_representations\n",
    "            for i in range(1, 5):\n",
    "                print(\"---- draw_representation 1\")\n",
    "                draw_representations(features, labels, k=4, seed=77, do_pca=True,\n",
    "                                     filename='/kaggle/working/dim_128_origin_{}.png'.format(i))\n",
    "\n",
    "        # influence = torch.FloatTensor(influence).cuda()\n",
    "        # features = torch.mm(influence, features)\n",
    "\n",
    "        features = sgc_precompute(features, adj, args.degree)\n",
    "\n",
    "        if args.vis == \"vis\":\n",
    "            #from plotFunc import draw_representations\n",
    "            for i in range(1, 5):\n",
    "                print(\"---- draw_representation 2\")\n",
    "                draw_representations(features, labels, k=4, seed=77, do_pca=True,\n",
    "                                     filename='/kaggle/working/dim_128_sgc_{}.png'.format(i))\n",
    "            exit(1)\n",
    "\n",
    "        val_acc_best, epoch_best = train_regression(model, features[idx_train], labels[idx_train], features[idx_val],\n",
    "                                                    U_dev, classLatMedian, classLonMedian, userLocation, args.epochs,\n",
    "                                                    args.weight_decay, args.lr, args.patience, model_file)\n",
    "\n",
    "    if args.model == \"HGNN\":\n",
    "        print(\"---- model == HGNN 1\")\n",
    "        val_acc_best, epoch_best = train_regression2(model, features[idx_train], labels[idx_train], features[idx_val],\n",
    "                                                     U_dev, classLatMedian, classLonMedian, userLocation,\n",
    "                                                     args.epochs, args.weight_decay, args.lr, args.patience,\n",
    "                                                     model_file, cluster_nodes, cluster_adj, node2cluster_arr)\n",
    "\n",
    "    # load model from file and test the model\n",
    "    # my_model = get_model(args.model, features.shape[1], labels.max().item() + 1, usecuda=args.usecuda)\n",
    "    # my_model.load_state_dict(torch.load(model_file))\n",
    "    print(\"---- load model from file and test the model\")\n",
    "    my_model = torch.load(model_file)\n",
    "    csv_file_test = \"/kaggle/working/dim_{}_{}_{}_{}_test.csv\".format(\n",
    "        features.shape[1], args.feature_norm, args.weight_decay, args.lr)\n",
    "    csv_file_train = \"/kaggle/working/dim_{}_{}_{}_{}_train.csv\".format(\n",
    "        features.shape[1], args.feature_norm, args.weight_decay, args.lr)\n",
    "    if args.model == \"HGNN\":\n",
    "        print(\"---- model == HGNN 2\")\n",
    "        meanDis, MedianDis, accAT161, distances, latlon_true, latlon_pred = geo_eval2(my_model, features[idx_test],\n",
    "                                                                                      U_test, classLatMedian,\n",
    "                                                                                      classLonMedian, userLocation,\n",
    "                                                                                      node2cluster_arr[idx_test])\n",
    "        # save_coordinate_true_predict(distances, latlon_true, latlon_pred, labels[idx_test], classLatMedian, classLonMedian,\n",
    "        #                              csv_file_test)\n",
    "\n",
    "        _, _, train_acc_at_161, distances, latlon_true, latlon_pred = geo_eval2(my_model, features[idx_train], U_train,\n",
    "                                                                                classLatMedian, classLonMedian,\n",
    "                                                                                userLocation, node2cluster_arr[idx_train])\n",
    "\n",
    "    else:\n",
    "        meanDis, MedianDis, accAT161, distances, latlon_true, latlon_pred = geo_eval(my_model, features[idx_test],\n",
    "                                                                                     U_test,\n",
    "                                                                                     classLatMedian, classLonMedian,\n",
    "                                                                                     userLocation)\n",
    "        # save_coordinate_true_predict(distances, latlon_true, latlon_pred, labels[idx_test], classLatMedian, classLonMedian,\n",
    "        #                              csv_file_test)\n",
    "\n",
    "        _, _, train_acc_at_161, distances, latlon_true, latlon_pred = geo_eval(my_model, features[idx_train], U_train,\n",
    "                                                                               classLatMedian, classLonMedian,\n",
    "                                                                               userLocation)\n",
    "\n",
    "    print(\"train_acc_at_161:{}\\t\\tTest:\\tMean:{}\\t\\tMedian:{}\\t\\tAcc@161:{}\\n\"\n",
    "          .format(train_acc_at_161, meanDis, MedianDis, accAT161))\n",
    "    # save_coordinate_true_predict(distances, latlon_true, latlon_pred, labels[idx_train], classLatMedian, classLonMedian,\n",
    "    #                              csv_file_train)\n",
    "\n",
    "    # write time, args and results down to file, format is important.\n",
    "    timeStr = time.strftime(\"%Y-%m-%d %H:%M:%S\\t\", time.localtime(time.time()))\n",
    "    argsStr = \"-dump_file:{}\\t-degree:{}\\t-lr:{}\\t-decay:{}\".format(\n",
    "        args.dump_file, args.degree, args.lr, args.weight_decay)\n",
    "    resultStr = \"Dev:\\tepoch_best:{}\\t\\tval_acc_best:{}\\n\".format(epoch_best, val_acc_best) + \\\n",
    "                \"train_acc_at_161:{}\\t\\tTest:\\tMean:{}\\t\\tMedian:{}\\t\\tAcc@161:{}\".format(\n",
    "                    train_acc_at_161, meanDis, MedianDis, accAT161)\n",
    "    content = \"\\n\" + timeStr + \"\\n\" + argsStr + \"\\n\" + resultStr + \"\\n\"\n",
    "    with open('/kaggle/working/influ_doc128_sgc.txt', 'a') as f:\n",
    "        f.write(content)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def save_coordinate_true_predict(distances, latlon_true, latlon_pred, labels, classLatMedian, classLonMedian,\n",
    "                                 coordinate_file):\n",
    "    \"\"\"\n",
    "    :return: record the true and false users.\n",
    "    \"\"\"\n",
    "    true_array, pred_array, dis_array = np.array(latlon_true), np.array(latlon_pred), np.array(distances)\n",
    "    dis_array = dis_array.reshape(len(dis_array), 1)\n",
    "    labels = labels.data.cpu().numpy().tolist()\n",
    "    lab_list = list()\n",
    "    for l in labels:\n",
    "        lat_lab, lon_lab = classLatMedian[str(l)], classLonMedian[str(l)]\n",
    "        lab_list.append([lat_lab, lon_lab, l])\n",
    "    lab_array = np.array(lab_list)\n",
    "    combine = np.hstack((true_array, pred_array, lab_array, dis_array))\n",
    "    out_str = \"id,true_lat,true_lon,pred_lat,pred_lon,pred_class,lab_lat,lab_lon,lab_class,distance,class_acc,acc@161\\n\"\n",
    "    with open(coordinate_file, 'w') as f:\n",
    "        for i, coor in enumerate(combine):\n",
    "            if coor[4] == coor[7]:\n",
    "                sign = \"Yes\"\n",
    "            else:\n",
    "                sign = \"No\"\n",
    "            if coor[8] <= 161:\n",
    "                sign2 = \"In.\"\n",
    "            else:\n",
    "                sign2 = \"Out\"\n",
    "            out_str += str(i) + ',' + str(coor[0]) + ',' + str(coor[1]) + ',' + str(coor[2]) + ',' + str(coor[3]) \\\n",
    "                       + ',' + str(coor[4]) + ',' + str(coor[5]) + ',' + str(coor[6]) + ',' + str(coor[7]) \\\n",
    "                       + ',' + str(coor[8]) + ',' + sign + ',' + sign2 + \"\\n\"\n",
    "            if (i % 1000 == 0) or (i == len(combine) - 1):\n",
    "                f.write(out_str)\n",
    "                out_str = \"\"\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd006faa",
   "metadata": {
    "papermill": {
     "duration": 0.015511,
     "end_time": "2024-09-10T16:39:25.676258",
     "exception": false,
     "start_time": "2024-09-10T16:39:25.660747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ece8032f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:25.710770Z",
     "iopub.status.busy": "2024-09-10T16:39:25.710297Z",
     "iopub.status.idle": "2024-09-10T16:39:25.716106Z",
     "shell.execute_reply": "2024-09-10T16:39:25.714601Z"
    },
    "papermill": {
     "duration": 0.025803,
     "end_time": "2024-09-10T16:39:25.718614",
     "exception": false,
     "start_time": "2024-09-10T16:39:25.692811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dd9e071",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:25.753541Z",
     "iopub.status.busy": "2024-09-10T16:39:25.753064Z",
     "iopub.status.idle": "2024-09-10T16:39:25.761280Z",
     "shell.execute_reply": "2024-09-10T16:39:25.759913Z"
    },
    "papermill": {
     "duration": 0.028893,
     "end_time": "2024-09-10T16:39:25.763812",
     "exception": false,
     "start_time": "2024-09-10T16:39:25.734919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function dir>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba19db34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:25.799173Z",
     "iopub.status.busy": "2024-09-10T16:39:25.798717Z",
     "iopub.status.idle": "2024-09-10T16:39:25.820870Z",
     "shell.execute_reply": "2024-09-10T16:39:25.819701Z"
    },
    "papermill": {
     "duration": 0.04336,
     "end_time": "2024-09-10T16:39:25.823715",
     "exception": false,
     "start_time": "2024-09-10T16:39:25.780355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from utils import sparse_mx_to_torch_sparse_tensor\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def sgc_precompute(features, adj, degree):\n",
    "    for i in range(degree):\n",
    "        features = torch.spmm(adj, features)\n",
    "    return features\n",
    "\n",
    "def parse_args(argv):\n",
    "    \"\"\"\n",
    "    Parse commandline arguments.\n",
    "    Arguments:\n",
    "        argv -- An argument list without the program name.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    print(parser)\n",
    "\n",
    "    # data_args: control the data loading\n",
    "    parser.add_argument('-dir', metavar='str', help='the detail directory of dataset',\n",
    "                        type=str, default='/kaggle/input/geotext')\n",
    "    parser.add_argument('-dump_file', metavar='str', help='the dir to load file include name',\n",
    "                        type=str, default='/kaggle/working/dump_doc_dim_128_for_hgnn.pkl')\n",
    "    parser.add_argument('-feature_norm', type=str, default='Standard') # choices=['None', 'Standard', 'Mean']\n",
    "\n",
    "    parser.add_argument('-doc2vec_model_file', metavar='str', help='the dir to load doc2vec_model_file .bin',\n",
    "                        type=str, default=\"/kaggle/input/gensim-doc2vec-model/model_dim_128_epoch_40.bin\")\n",
    "    parser.add_argument('-res_file', metavar='str', help='the dir to save the result file',\n",
    "                        type=str, default='./kaggle/working/res_cmu_hgnn_dim_128.txt')\n",
    "\n",
    "    # -dir ./data/cmu/ -bucket 50 -celebrity 5\n",
    "    # -dir ./data/na/ -bucket 2400 -celebrity 15\n",
    "    # -dir ./data/world/ -bucket 2400 -celebrity 5\n",
    "    parser.add_argument('-edge_dis_file', metavar='str', help='the dir to load all edges distance file',\n",
    "                        type=str, default='./my_assets/no_ues_now/edge_dis.pkl')\n",
    "    parser.add_argument('-bucket', metavar='int', help='discretisation bucket size', type=int, default=50)\n",
    "    parser.add_argument('-mindf', metavar='int', help='minimum document frequency in BoW', type=int, default=10)\n",
    "    parser.add_argument('-encoding', metavar='str', help='Data Encoding (e.g.latin1, utf-8)', type=str,\n",
    "                        default='latin1')\n",
    "    parser.add_argument('-celebrity', metavar='int', help='celebrity threshold', type=int, default=5)\n",
    "    parser.add_argument('-vis', metavar='str', help='visualise representations', type=str, default=None)\n",
    "    parser.add_argument('-builddata', action='store_true', help='if true do not recreated dumped data', default=False)\n",
    "\n",
    "    # process_args: control the data preprocess\n",
    "    parser.add_argument('-degree', type=int, help='degree of the approximation.', default=2)\n",
    "    parser.add_argument('-normalization', type=str, help='Normalization method for the adjacency matrix.',\n",
    "                        choices=['NormLap', 'Lap', 'RWalkLap', 'FirstOrderGCN', 'AugNormAdj', 'NormAdj',\n",
    "                                 'RWalk', 'AugRWalk', 'NoNorm'], default='AugNormAdj')\n",
    "\n",
    "    # model_args: the hyper-parameter of model\n",
    "    parser.add_argument('-model', type=str, help='model to use.', default=\"HGNN\")\n",
    "    parser.add_argument('-usecuda', action='store_true', help='Use CUDA for training.', default=False)\n",
    "    parser.add_argument('-seed', metavar='int', help='random seed', type=int, default=77)\n",
    "    parser.add_argument('-epochs', type=int, help='Number of epochs to train.', default=30000)\n",
    "    parser.add_argument('-lr', type=float, help='Initial learning rate.', default=0.001)\n",
    "    parser.add_argument('-weight_decay', type=float, help='Weight decay (L2 loss on parameters).', default=5e-7)\n",
    "    parser.add_argument('-patience', help='max iter for early stopping', type=int, default=30)\n",
    "    parser.add_argument('-batch', metavar='int', help='SGD batch size', type=int, default=1024)\n",
    "\n",
    "    print(parser)\n",
    "    args, unknown = parser.parse_known_args(argv)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d89bc9d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:25.860216Z",
     "iopub.status.busy": "2024-09-10T16:39:25.859780Z",
     "iopub.status.idle": "2024-09-10T16:39:25.867255Z",
     "shell.execute_reply": "2024-09-10T16:39:25.866087Z"
    },
    "papermill": {
     "duration": 0.028985,
     "end_time": "2024-09-10T16:39:25.869753",
     "exception": false,
     "start_time": "2024-09-10T16:39:25.840768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-f', '/tmp/tmppp4z0o9k.json', '--HistoryManager.hist_file=:memory:']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de8cb521",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:25.905167Z",
     "iopub.status.busy": "2024-09-10T16:39:25.904747Z",
     "iopub.status.idle": "2024-09-10T16:39:25.912579Z",
     "shell.execute_reply": "2024-09-10T16:39:25.911358Z"
    },
    "papermill": {
     "duration": 0.029005,
     "end_time": "2024-09-10T16:39:25.915726",
     "exception": false,
     "start_time": "2024-09-10T16:39:25.886721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)\n",
      "ArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)\n"
     ]
    }
   ],
   "source": [
    "args = parse_args(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa16b54a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:39:25.950774Z",
     "iopub.status.busy": "2024-09-10T16:39:25.950330Z",
     "iopub.status.idle": "2024-09-10T16:42:51.303475Z",
     "shell.execute_reply": "2024-09-10T16:42:51.302358Z"
    },
    "papermill": {
     "duration": 205.374133,
     "end_time": "2024-09-10T16:42:51.306298",
     "exception": false,
     "start_time": "2024-09-10T16:39:25.932165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree:2\t\tweight_decay:5e-07\t\tlr:0.0001\n",
      "----preprocess_data\n",
      "loading the dataset from: /kaggle/input/geotext\n",
      "# the number of clusterer labels is: 129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/691640839.py:232: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  feature_list.append(model.docvecs[i])\n",
      "/tmp/ipykernel_17/691640839.py:240: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  feature_list.append(model.docvecs[i])\n",
      "/tmp/ipykernel_17/691640839.py:248: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  feature_list.append(model.docvecs[i])\n",
      "/tmp/ipykernel_17/691640839.py:74: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mentions = [m.lower() for m in pattern.findall(self.df_train.text[i])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training    n_samples: 5685, n_features: 128\n",
      "development n_samples: 1895, n_features: 128\n",
      "test        n_samples: 1895, n_features: 128\n",
      "start adding the train graph\n",
      "start adding the dev graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/691640839.py:93: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mentions = [m.lower() for m in pattern.findall(self.df_dev.text[i])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start adding the test graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/691640839.py:112: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  mentions = [m.lower() for m in pattern.findall(self.df_test.text[i])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nodes: 128269, #edges: 193185\n",
      "removing 91477 celebrity nodes with degree higher than 5\n",
      "projecting the graph\n",
      "#nodes: 9475, #edges: 77155\n",
      "adjacency matrix created.\n",
      "---- get training node index of each set.\n",
      "---- build the cluster graph Adjacency Matrix.\n",
      "---- calculate the shortest hop path of every pair of nodes on the mention graph.\n",
      "---- num_of_nodes 9475\n",
      "1000 shortest path done.\n",
      "2000 shortest path done.\n",
      "3000 shortest path done.\n",
      "4000 shortest path done.\n",
      "5000 shortest path done.\n",
      "6000 shortest path done.\n",
      "7000 shortest path done.\n",
      "8000 shortest path done.\n",
      "9000 shortest path done.\n",
      "calculate the shortest hop path on the mention graph.\n",
      "1000 have gotten the shortest path.\n",
      "2000 have gotten the shortest path.\n",
      "3000 have gotten the shortest path.\n",
      "4000 have gotten the shortest path.\n",
      "5000 have gotten the shortest path.\n",
      "6000 have gotten the shortest path.\n",
      "7000 have gotten the shortest path.\n",
      "8000 have gotten the shortest path.\n",
      "9000 have gotten the shortest path.\n",
      "save into files in order to repeat calculate.\n",
      "successfully dump data in /kaggle/working/dump_doc_dim_128_for_hgnn.pkl\n",
      "----process_data\n",
      "none feature normalization.\n",
      "feature shape:torch.Size([9475, 128])\n",
      "----Multiple assignement\n",
      "----get_model\n",
      "HGNN model starting...\n",
      "----model_file\n",
      "---- model == HGNN 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/723670895.py:10: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:641.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop! \t epoch_best:1\t \t \tval_acc_best:31.87335092348285\n",
      "---- load model from file and test the model\n",
      "---- model == HGNN 2\n",
      "train_acc_at_161:32.01407211961302\t\tTest:\tMean:1139.7981084893436\t\tMedian:727.1904795876078\t\tAcc@161:31.820580474934037\n",
      "\n",
      "degree:2\t\tweight_decay:5e-07\t\tlr:0.0001\n",
      "----preprocess_data\n",
      "loading data from file : /kaggle/working/dump_doc_dim_128_for_hgnn.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/4104364595.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  my_model = torch.load(model_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----process_data\n",
      "none feature normalization.\n",
      "feature shape:torch.Size([9475, 128])\n",
      "----Multiple assignement\n",
      "----get_model\n",
      "HGNN model starting...\n",
      "----model_file\n",
      "---- model == HGNN 1\n",
      "Early stop! \t epoch_best:1\t \t \tval_acc_best:31.87335092348285\n",
      "---- load model from file and test the model\n",
      "---- model == HGNN 2\n",
      "train_acc_at_161:32.04925241864556\t\tTest:\tMean:1138.2004468100583\t\tMedian:726.4844078251912\t\tAcc@161:31.820580474934037\n",
      "\n",
      "degree:2\t\tweight_decay:5e-07\t\tlr:0.001\n",
      "----preprocess_data\n",
      "loading data from file : /kaggle/working/dump_doc_dim_128_for_hgnn.pkl\n",
      "----process_data\n",
      "none feature normalization.\n",
      "feature shape:torch.Size([9475, 128])\n",
      "----Multiple assignement\n",
      "----get_model\n",
      "HGNN model starting...\n",
      "----model_file\n",
      "---- model == HGNN 1\n",
      "Early stop! \t epoch_best:13\t \t \tval_acc_best:31.926121372031663\n",
      "---- load model from file and test the model\n",
      "---- model == HGNN 2\n",
      "train_acc_at_161:32.119613016710645\t\tTest:\tMean:1143.928902303828\t\tMedian:739.1598728036248\t\tAcc@161:31.767810026385224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "degree_rate = [2]\n",
    "weight_decay_rate = [5e-7]\n",
    "lr_rate = [0.0001, 0.0001, 0.001]\n",
    "for degree in degree_rate:\n",
    "    args.degree = degree\n",
    "    for weight_decay in weight_decay_rate:\n",
    "        args.weight_decay = weight_decay\n",
    "        for lr in lr_rate:\n",
    "            args.lr = lr\n",
    "            print(\"degree:{}\\t\\tweight_decay:{}\\t\\tlr:{}\".format(args.degree, args.weight_decay, args.lr))\n",
    "            main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43dbee0",
   "metadata": {
    "papermill": {
     "duration": 0.020326,
     "end_time": "2024-09-10T16:42:51.347181",
     "exception": false,
     "start_time": "2024-09-10T16:42:51.326855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be697084",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:42:51.391075Z",
     "iopub.status.busy": "2024-09-10T16:42:51.389529Z",
     "iopub.status.idle": "2024-09-10T16:42:51.395051Z",
     "shell.execute_reply": "2024-09-10T16:42:51.393876Z"
    },
    "papermill": {
     "duration": 0.029356,
     "end_time": "2024-09-10T16:42:51.397470",
     "exception": false,
     "start_time": "2024-09-10T16:42:51.368114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = gensim.models.Doc2Vec.load(\"/kaggle/input/gensim-doc2vec-model/model_dim_128_epoch_40.bin\")\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f028d56",
   "metadata": {
    "papermill": {
     "duration": 0.019933,
     "end_time": "2024-09-10T16:42:51.437514",
     "exception": false,
     "start_time": "2024-09-10T16:42:51.417581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3394583,
     "sourceId": 9344214,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 195328175,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 273.704387,
   "end_time": "2024-09-10T16:42:53.966139",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-10T16:38:20.261752",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
